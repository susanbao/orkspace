{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf872ef-b687-4fc4-a6bb-4c77bdfc6826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler, random_split, TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a12d68-b893-454d-9fdd-f320a3e587de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "model_data_path = \"./data/5_scale_31/\"\n",
    "data_path = model_data_path + split + \"/data/\"\n",
    "summary_annotation_file = model_data_path + split + \"/summary.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ccf01a-82dc-4ea7-96f7-4dbbcfd652dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pred_logits', 'pred_boxes', 'aux_outputs', 'interm_outputs', 'interm_outputs_for_matching_pre', 'dn_meta'])\n",
      "dict_keys(['loss', 'loss_bbox_dn', 'loss_giou_dn', 'loss_ce_dn', 'loss_ce', 'loss_bbox', 'loss_giou', 'loss_ce_0', 'loss_bbox_0', 'loss_giou_0', 'loss_bbox_dn_0', 'loss_giou_dn_0', 'loss_ce_dn_0', 'loss_ce_1', 'loss_bbox_1', 'loss_giou_1', 'loss_bbox_dn_1', 'loss_giou_dn_1', 'loss_ce_dn_1', 'loss_ce_2', 'loss_bbox_2', 'loss_giou_2', 'loss_bbox_dn_2', 'loss_giou_dn_2', 'loss_ce_dn_2', 'loss_ce_3', 'loss_bbox_3', 'loss_giou_3', 'loss_bbox_dn_3', 'loss_giou_dn_3', 'loss_ce_dn_3', 'loss_ce_4', 'loss_bbox_4', 'loss_giou_4', 'loss_bbox_dn_4', 'loss_giou_dn_4', 'loss_ce_dn_4', 'loss_ce_interm', 'loss_bbox_interm', 'loss_giou_interm', 'loss_bbox_dn_unscaled', 'loss_giou_dn_unscaled', 'loss_ce_dn_unscaled', 'loss_xy_dn_unscaled', 'loss_hw_dn_unscaled', 'cardinality_error_dn_unscaled', 'loss_ce_unscaled', 'class_error_unscaled', 'loss_bbox_unscaled', 'loss_giou_unscaled', 'loss_xy_unscaled', 'loss_hw_unscaled', 'cardinality_error_unscaled', 'loss_ce_0_unscaled', 'loss_bbox_0_unscaled', 'loss_giou_0_unscaled', 'loss_xy_0_unscaled', 'loss_hw_0_unscaled', 'cardinality_error_0_unscaled', 'loss_bbox_dn_0_unscaled', 'loss_giou_dn_0_unscaled', 'loss_ce_dn_0_unscaled', 'loss_xy_dn_0_unscaled', 'loss_hw_dn_0_unscaled', 'cardinality_error_dn_0_unscaled', 'loss_ce_1_unscaled', 'loss_bbox_1_unscaled', 'loss_giou_1_unscaled', 'loss_xy_1_unscaled', 'loss_hw_1_unscaled', 'cardinality_error_1_unscaled', 'loss_bbox_dn_1_unscaled', 'loss_giou_dn_1_unscaled', 'loss_ce_dn_1_unscaled', 'loss_xy_dn_1_unscaled', 'loss_hw_dn_1_unscaled', 'cardinality_error_dn_1_unscaled', 'loss_ce_2_unscaled', 'loss_bbox_2_unscaled', 'loss_giou_2_unscaled', 'loss_xy_2_unscaled', 'loss_hw_2_unscaled', 'cardinality_error_2_unscaled', 'loss_bbox_dn_2_unscaled', 'loss_giou_dn_2_unscaled', 'loss_ce_dn_2_unscaled', 'loss_xy_dn_2_unscaled', 'loss_hw_dn_2_unscaled', 'cardinality_error_dn_2_unscaled', 'loss_ce_3_unscaled', 'loss_bbox_3_unscaled', 'loss_giou_3_unscaled', 'loss_xy_3_unscaled', 'loss_hw_3_unscaled', 'cardinality_error_3_unscaled', 'loss_bbox_dn_3_unscaled', 'loss_giou_dn_3_unscaled', 'loss_ce_dn_3_unscaled', 'loss_xy_dn_3_unscaled', 'loss_hw_dn_3_unscaled', 'cardinality_error_dn_3_unscaled', 'loss_ce_4_unscaled', 'loss_bbox_4_unscaled', 'loss_giou_4_unscaled', 'loss_xy_4_unscaled', 'loss_hw_4_unscaled', 'cardinality_error_4_unscaled', 'loss_bbox_dn_4_unscaled', 'loss_giou_dn_4_unscaled', 'loss_ce_dn_4_unscaled', 'loss_xy_dn_4_unscaled', 'loss_hw_dn_4_unscaled', 'cardinality_error_dn_4_unscaled', 'loss_ce_interm_unscaled', 'loss_bbox_interm_unscaled', 'loss_giou_interm_unscaled', 'loss_xy_interm_unscaled', 'loss_hw_interm_unscaled', 'cardinality_error_interm_unscaled'])\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + \"0.json\", \"r\") as outfile:\n",
    "    data = json.load(outfile)\n",
    "print(data['input'].keys())\n",
    "print(data['annotation'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64955065-1cd4-4db1-91aa-8b16f4dea443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_one_key_from_file(file, first_key, second_key, third_key=None):\n",
    "#     with open(file, \"r\") as outfile:\n",
    "#         data = json.load(outfile)\n",
    "#     return data[first_key][second_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e36b7-3ad0-46bb-bb98-395337565bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files = os.listdir(data_path)\n",
    "# max_file_num = 10000\n",
    "# files_num = min(len(files), max_file_num)\n",
    "# # assert len(files) == files_num\n",
    "# inputs = []\n",
    "# annotations = []\n",
    "# for file_idx in range(files_num):\n",
    "#     file_path = data_path + str(file_idx) + \".json\"\n",
    "#     with open(file_path, \"r\") as outfile:\n",
    "#         data = json.load(outfile)\n",
    "#         pred_logits = np.squeeze(np.array(data['input']['pred_logits']), axis=0)\n",
    "#         pred_boxes = np.squeeze(np.array(data['input']['pred_boxes']), axis=0)\n",
    "#         pred_input = np.concatenate((pred_logits, pred_boxes), axis=1)\n",
    "#         inputs.append(pred_input.tolist())\n",
    "#         loss_ce = data['annotation']['loss_ce']\n",
    "#         loss_bbox = data['annotation']['loss_bbox']\n",
    "#         loss_giou = data['annotation']['loss_giou']\n",
    "#         annotations.append(loss_ce + loss_bbox + loss_giou)\n",
    "#     if file_idx % 1000 == 0:\n",
    "#         print(file_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6bd489-0d6d-473c-8f66-4268b63c08de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store_preprocess_inputs_path = model_data_path + split + f\"/pre_data/{split}_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, np.array(inputs))\n",
    "# store_preprocess_annotations_path = model_data_path + split + f\"/pre_data/{split}_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, np.array(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52fd912-3c43-4661-9cb1-9ec69c883141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "119869e3-f1d6-4f98-9ec5-73cbb774fd68",
   "metadata": {},
   "source": [
    "## MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c792756-c148-4f24-ac03-2230f97e29be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MPLNet(nn.Module):\n",
    "    def __init__(self, input_dims = 10000, output_dims = 1, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer1=nn.Linear(input_dims, 10000)\n",
    "        self.layer2=nn.Linear(10000, 1000)\n",
    "        self.layer3=nn.Linear(1000, 100)\n",
    "        self.layer4=nn.Linear(100, 10)\n",
    "        self.layer5=nn.Linear(10, 1)\n",
    "        self.norm1 = nn.LayerNorm(10000)\n",
    "        self.norm2 = nn.LayerNorm(1000)\n",
    "        self.norm3 = nn.LayerNorm(100)\n",
    "        self.norm4 = nn.LayerNorm(10)\n",
    "        self._init_parms(self.layer1)\n",
    "        self._init_parms(self.layer2)\n",
    "        self._init_parms(self.layer3)\n",
    "        self._init_parms(self.layer4)\n",
    "        self._init_parms(self.layer5)\n",
    "        # self.dropout1 = nn.Dropout(dropout)\n",
    "        # self.dropout2 = nn.Dropout(dropout)\n",
    "        # self.dropout3 = nn.Dropout(dropout)\n",
    "        self.input_dims = input_dims\n",
    "    \n",
    "    def _init_parms(self, module):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, self.input_dims)\n",
    "        x=nn.functional.relu(self.norm1(self.layer1(x)))\n",
    "        x=nn.functional.relu(self.norm2(self.layer2(x)))\n",
    "        x=nn.functional.relu(self.norm3(self.layer3(x)))\n",
    "        x=nn.functional.relu(self.norm4(self.layer4(x)))\n",
    "        x=self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c1131a-98c2-4051-8937-3253c154604b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPLNet(\n",
       "  (layer1): Linear(in_features=85500, out_features=10000, bias=True)\n",
       "  (layer2): Linear(in_features=10000, out_features=1000, bias=True)\n",
       "  (layer3): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (layer4): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (layer5): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (norm1): LayerNorm((10000,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm4): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_name = \"cuda:1\"\n",
    "device = torch.device(device_name)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "rate_learning = 1e-4\n",
    "model = MPLNet(input_dims=85500)\n",
    "loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=rate_learning)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec897e5-8bfb-447b-8f9a-14907a2cd468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "batch_size = 10\n",
    "store_preprocess_inputs_path = model_data_path + split + f\"/pre_data/{split}_inputs.npy\"\n",
    "store_preprocess_annotations_path = model_data_path + split + f\"/pre_data/{split}_annotations.npy\"\n",
    "with open(store_preprocess_inputs_path, 'rb') as outfile:\n",
    "    inputs = torch.from_numpy(np.load(outfile))\n",
    "with open(store_preprocess_annotations_path, 'rb') as outfile:\n",
    "    annotations = torch.from_numpy(np.load(outfile))\n",
    "    \n",
    "datasets = TensorDataset(inputs, annotations)\n",
    "datasets_nums = inputs.shape[0]\n",
    "train_nums = int(datasets_nums * 0.7)\n",
    "train_datasets, val_datasets = torch.utils.data.random_split(datasets, [train_nums, datasets_nums-train_nums])\n",
    "train_sampler = torch.utils.data.RandomSampler(train_datasets)\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(train_sampler, batch_size, drop_last=True)\n",
    "train_dataloader = DataLoader(train_datasets, batch_sampler=batch_sampler_train)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_datasets)\n",
    "valid_dataloader = DataLoader(val_datasets, 1, sampler=val_sampler, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9532b4e8-5b7a-4c26-bdf3-b7268cd00894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1, 10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Step: 0, Loss 1.4079663753509521\n",
      "Train Epoch: 0, Step: 100, Loss 0.14491362869739532\n",
      "Train Epoch: 0, Step: 200, Loss 0.07064949721097946\n",
      "Train Epoch: 0, Step: 300, Loss 0.1600571721792221\n",
      "Train Epoch: 0, Step: 400, Loss 0.07113819569349289\n",
      "Train Epoch: 0, Step: 500, Loss 0.06636536866426468\n",
      "Train Epoch: 0, Step: 600, Loss 0.0470028817653656\n",
      "Epoch: 0\n",
      "Training loss: 0.13012654406151603\n",
      "Validation loss: 0.1096319569219806\n",
      "Train Epoch: 1, Step: 0, Loss 0.09794391691684723\n",
      "Train Epoch: 1, Step: 100, Loss 0.07985158264636993\n",
      "Train Epoch: 1, Step: 200, Loss 0.05286634340882301\n",
      "Train Epoch: 1, Step: 300, Loss 0.0969143733382225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4209/2179073195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Epoch: {}, Step: {}, Loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    # Performing Training for each epoch\n",
    "    training_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    # The training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input, label = batch\n",
    "        input = input.to(device=device_name).type(torch.float)\n",
    "        label = label.to(device=device_name).type(torch.float)[None, :]\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print(\"Train Epoch: {}, Step: {}, Loss {}\".format(epoch, step, loss.item()))\n",
    "\n",
    "\n",
    "    # Performing Validation for each epoch\n",
    "    validation_loss = 0.\n",
    "    model.eval()\n",
    "\n",
    "    # The validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        input, label = batch\n",
    "        input = input.to(device=device_name).type(torch.float)\n",
    "        label = label.to(device=device_name).type(torch.float)[None, :]\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "    # Calculating the average training and validation loss over epoch\n",
    "    training_loss_avg = training_loss/len(train_dataloader)\n",
    "    validation_loss_avg = validation_loss/len(valid_dataloader)\n",
    "\n",
    "    # Printing average training and average validation losses\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    print(\"Training loss: {}\".format(training_loss_avg))\n",
    "    print(\"Validation loss: {}\".format(validation_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd72ffe-7191-409d-a02d-7aaaf0dd1d66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46378481 0.50836468 0.43675882 0.47644717 0.4669866  0.49336869\n",
      " 0.52880955 0.41137904 0.45204777 0.5399906  0.44983721 0.49561977\n",
      " 0.51709634 0.54614073 0.47704571 0.49241424 0.49363971 0.45553952\n",
      " 0.4987483  0.41566736 0.48889589 0.4868024  0.50882953 0.48026282\n",
      " 0.40693206 0.50247908 0.49741501 0.46098775 0.44397908 0.41126758\n",
      " 0.50637001 0.51956153 0.47695261 0.47220343 0.50901115 0.51671141\n",
      " 0.45579845 0.46015316 0.54054099 0.54043818 0.55663884 0.53353608\n",
      " 0.46967125 0.47010213 0.51379019 0.42288727 0.46069342 0.47276914\n",
      " 0.48792249 0.48977292 0.45354086 0.51540637 0.53348166 0.47597975\n",
      " 0.45855373 0.44176763 0.39752394 0.45308101 0.46009493 0.44475228\n",
      " 0.46692824 0.50908798 0.56615192 0.49608469 0.44961447 0.52036077\n",
      " 0.57642484 0.5131554  0.40966755 0.46586293 0.50519437 0.53614908\n",
      " 0.45775819 0.45283502 0.48057812 0.43737203 0.5045436  0.45854819\n",
      " 0.50428152 0.42545217 0.46880782 0.47415984 0.43131417 0.52466351\n",
      " 0.44499087 0.44779795 0.49688888 0.46387064 0.4989056  0.5235247\n",
      " 0.47418368 0.48410743 0.48046452 0.47268873 0.45790619 0.46847296\n",
      " 0.52941495 0.45349163 0.47023422 0.46800601]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "result_example = np.zeros(100)\n",
    "for i in range(100):\n",
    "    input = inputs[i].to(device=device_name).type(torch.float)\n",
    "    output = model(input)\n",
    "    result_example[i] = output.detach().cpu().numpy()\n",
    "print(result_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f112db-b1fd-4687-becd-4c1ab000c077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d73f8bf-d873-4acf-9d86-68e97fa45430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ckpts/5_scale_31_mlp.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b2f33-8c33-422c-91c4-70cca4efc0b9",
   "metadata": {},
   "source": [
    "## Active testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50aa81cb-cf60-4f45-a23d-5c1400ed9ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights\n",
    "\n",
    "def get_expected_loss_MLP(inputs):\n",
    "    model.eval()\n",
    "    datasets_nums = inputs.shape[0]\n",
    "    expected_loss = np.zeros(datasets_nums)\n",
    "    for i in range(datasets_nums):\n",
    "        input = inputs[i].to(device=device_name).type(torch.float)\n",
    "        output = model(input)\n",
    "        expected_loss[i] = output.detach().cpu().numpy()\n",
    "    return expected_loss\n",
    "\n",
    "def run_one_approximate_risk_estimator(true_losses, expected_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk\n",
    "\n",
    "def get_true_loss(annotations):\n",
    "    return annotations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f87653-7e15-4230-b862-0fa567a3eaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_split = \"val\"\n",
    "store_preprocess_inputs_path = model_data_path + val_split + f\"/pre_data/{val_split}_inputs.npy\"\n",
    "store_preprocess_annotations_path = model_data_path + val_split + f\"/pre_data/{val_split}_annotations.npy\"\n",
    "with open(store_preprocess_inputs_path, 'rb') as outfile:\n",
    "    val_inputs = torch.from_numpy(np.load(outfile))\n",
    "with open(store_preprocess_annotations_path, 'rb') as outfile:\n",
    "    val_annotations = torch.from_numpy(np.load(outfile))\n",
    "true_losses = get_true_loss(val_annotations)\n",
    "expected_losses = get_expected_loss_MLP(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb087e9e-4659-42fa-a3fa-6cbbcff2a589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_json_path = \"./results/MPL_loss_compare/\"\n",
    "sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7245d656-1866-46c7-9bfd-88be3adfc47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"MLP_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"MLP\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_approximate_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7b722-f249-4594-8ba7-48f3e5921b88",
   "metadata": {},
   "source": [
    "## Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "722997e5-2101-4909-8ac0-ff3cc42b5c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "290436a8-ec75-4ec2-8918-a9cf5c395177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(true_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26b4becf-987f-4bbc-bf7b-51ec5ecf457b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator_LURE(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    weights = np.ones(samples_num) * 1.0 / samples_num\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, true_losses.size)\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "822bb53c-a5fe-4ae7-8ea9-d3e89299c699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_LURE_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample LURE\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator_LURE(true_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b271f4-eb67-4279-a6a6-68096a4d2bde",
   "metadata": {},
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff46eefe-cf3c-48fa-8d69-66c53621921a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c094959f-4adb-4e05-bdb3-c3d99acac115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": inputs.shape[0]}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(true_losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935c059-8c94-4a19-9ec2-f446480601ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
