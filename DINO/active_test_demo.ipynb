{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a43f0d0-2e5a-4965-8102-b3223c9aecbe",
   "metadata": {},
   "source": [
    "# Demo on active testing from paper\n",
    "Active Testing: Sample–Efficient Model Evaluation\n",
    "\n",
    "Active Surrogate Estimators: An Active Learning Approach to Label–Efficient Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61163ae3-dd93-4034-9114-e4ae9c61551d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch, json\n",
    "import numpy as np\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a39da7-eafe-4d24-a4d4-0a4e8a747d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_path = \"./results/deep_ensemble/\"\n",
    "consider_model_results = {\"DINO_0011_4scale.pkl\", \"DINO_0011_5scale.pkl\", \"DINO_0023_4scale.pkl\", \"DINO_0022_5scale.pkl\", \"DINO_0033_4scale.pkl\", \"DINO_0031_5scale.pkl\"}\n",
    "ego_model_index = 5\n",
    "model_checkpoint_path = \"ckpts/checkpoint0031_5scale.pth\" # change the path of the model checkpoint\n",
    "result_json_path = \"./results/surr_ensemble_compare/\"\n",
    "model_nums = len(consider_model_results)\n",
    "sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188a666-78f5-478b-bbb7-60e7d90b71bb",
   "metadata": {},
   "source": [
    "## Load model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0d6f44-4e60-4672-9003-1e5619362cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ['pred_boxes', 'pred_logits', 'scores', 'labels', 'boxes']\n",
    "deep_ensemble_results = []\n",
    "for path in consider_model_results:\n",
    "    temp_path = result_path + path\n",
    "    with open(temp_path, \"rb\") as outfile:\n",
    "         deep_ensemble_results.append(pickle.load(outfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f347456-ff77-4ab6-a3e9-fb5d41e34479",
   "metadata": {},
   "source": [
    "## Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f19a00-acb5-4aff-8f66-3f32f13df41f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_aug_params: {\n",
      "  \"scales\": [\n",
      "    480,\n",
      "    512,\n",
      "    544,\n",
      "    576,\n",
      "    608,\n",
      "    640,\n",
      "    672,\n",
      "    704,\n",
      "    736,\n",
      "    768,\n",
      "    800\n",
      "  ],\n",
      "  \"max_size\": 1333,\n",
      "  \"scales2_resize\": [\n",
      "    400,\n",
      "    500,\n",
      "    600\n",
      "  ],\n",
      "  \"scales2_crop\": [\n",
      "    384,\n",
      "    600\n",
      "  ]\n",
      "}\n",
      "loading annotations into memory...\n",
      "Done (t=2.71s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "scale_config = \"5scale\"\n",
    "model_config_path = \"config/DINO/DINO_\" + scale_config + \".py\" # change the path of the model config file\n",
    "args = SLConfig.fromfile(model_config_path) \n",
    "args.device = 'cuda' \n",
    "args.dataset_file = 'coco'\n",
    "args.coco_path = \"../coco/\" # the path of coco\n",
    "args.fix_size = False\n",
    "dataset_val = build_dataset(image_set='val', args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023448a6-871b-46a0-8b51-8a2ff89c80c2",
   "metadata": {},
   "source": [
    "## Model results postprocess\n",
    "Include: remove prediction with low score, prediction matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ad69ae-39fa-463a-9eda-13e3362abca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_dict = ['pred_boxes', 'pred_logits', 'scores', 'labels', 'boxes']\n",
    "score_threshold = 0\n",
    "max_num_select = 300\n",
    "pro_results = copy.deepcopy(deep_ensemble_results)\n",
    "for agent in range(model_nums):\n",
    "    for img_idx in range(len(pro_results[agent])):\n",
    "        pred_logits = pro_results[agent][img_idx]['pred_logits']\n",
    "        pred_boxes = pro_results[agent][img_idx]['pred_boxes']\n",
    "        if pred_logits is None:\n",
    "            continue\n",
    "        # only consider prediction with score larger than score_threshold\n",
    "        select_mask = pred_logits > score_threshold\n",
    "        select_idx_all = np.reshape(select_mask, -1).nonzero()[0]\n",
    "        select_idx = select_idx_all // pred_logits.shape[2]\n",
    "        assert len(select_idx) <= max_num_select\n",
    "        if len(select_idx) <= 0:\n",
    "            pro_results[agent][img_idx]['pred_logits']  = None\n",
    "            pro_results[agent][img_idx]['lables']  = None\n",
    "            pro_results[agent][img_idx]['scores']  = None\n",
    "            pro_results[agent][img_idx]['pred_boxes']  = None\n",
    "            continue\n",
    "        lables = select_idx_all % pred_logits.shape[2]\n",
    "        scores = pred_logits[select_mask]\n",
    "        pred_boxes = pred_boxes[:, select_idx]\n",
    "        pred_logits = pred_logits[:, select_idx]\n",
    "        pro_results[agent][img_idx]['pred_logits'] = torch.from_numpy(pred_logits)\n",
    "        pro_results[agent][img_idx]['lables'] = torch.from_numpy(lables)\n",
    "        pro_results[agent][img_idx]['scores'] = torch.from_numpy(scores)\n",
    "        pro_results[agent][img_idx]['pred_boxes'] = torch.from_numpy(pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1afab6-76cd-40e3-9619-2d5fb118d1da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4]),\n",
       " torch.Size([1, 8, 91]),\n",
       " torch.Size([8]),\n",
       " torch.Size([8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_results[0][0]['pred_boxes'].shape, pro_results[0][0]['pred_logits'].shape, pro_results[0][0]['lables'].shape, pro_results[0][0]['scores'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f2a164-cd20-443e-b075-3366f3161b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "def hungarian_matching(outputs, targets, cost_class = 2.0, cost_bbox = 5.0, cost_giou = 2.0, focal_alpha = 0.25, cost_threshold = 2):\n",
    "    \"\"\" Performs the matching\n",
    "    Params:\n",
    "        outputs/targets: This is a dict that contains at least these entries:\n",
    "             \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits, batch_size = 1\n",
    "             \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "             \"lables\": Tensor of dim [num_queries] with the label of each predicted box\n",
    "        cost_threshold: threshold for distance between \n",
    "    Returns:\n",
    "        A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "            - index_i is the indices of the selected predictions (in order)\n",
    "            - index_j is the indices of the corresponding selected targets (in order with high priority)\n",
    "        For each batch element, it holds:\n",
    "            len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "    \"\"\"\n",
    "    if outputs is None or targets is None or outputs[\"pred_logits\"] is None or targets[\"pred_logits\"] is None:\n",
    "        return []\n",
    "    assert outputs[\"pred_logits\"].shape[0] == targets[\"pred_logits\"].shape[0]\n",
    "    bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "    assert bs == 1\n",
    "    \n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    out_prob = outputs[\"pred_logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n",
    "    out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "    \n",
    "    tgt_ids = targets[\"lables\"]\n",
    "    tgt_bbox = targets[\"pred_boxes\"].flatten(0, 1)\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox + cost_class * cost_class + cost_giou * cost_giou\n",
    "    C = C.view(num_queries, -1)\n",
    "    \n",
    "    # num_target_boxes = targets[\"pred_logits\"].shape[1]\n",
    "    # indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(num_target_boxes, -1))]\n",
    "    indices = linear_sum_assignment(C)\n",
    "    indices = [indices[0][np.argsort(indices[1])], np.sort(indices[1])]\n",
    "    select_mask = [True if C[indices[0][i],indices[1][i]] <= cost_threshold else False for i in range(len(indices[0]))]\n",
    "    # return [torch.as_tensor(indices[0][select_mask], dtype=torch.int64), torch.as_tensor(indices[1][select_mask], dtype=torch.int64)], C, cost_class, cost_bbox, cost_giou\n",
    "    # return [torch.as_tensor(indices[0][select_mask], dtype=torch.int64), torch.as_tensor(indices[1][select_mask], dtype=torch.int64)]\n",
    "    return [indices[0][select_mask], indices[1][select_mask]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa419e5-9dd0-4b9d-ad26-bde7559fb4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3, 2, 4, 5, 7, 6, 0]), array([0, 1, 2, 3, 4, 6, 7])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = hungarian_matching(pro_results[0][0], pro_results[5][0])\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa5c9a-af72-4ca4-b834-9523460dbdae",
   "metadata": {},
   "source": [
    "## Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0966ab81-82b6-4012-a976-4fa7e806ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference to ASE code AnySurrogateAcquisitionEntropy + LazySurrEnsemble\n",
    "def mean_surr_predict(surr_model_preds):\n",
    "    return np.mean(surr_model_preds, axis=1)\n",
    "\n",
    "def entropy_loss(surr_model_preds, model_preds):\n",
    "    surr_predicts = np.mean(surr_model_preds, axis=1)\n",
    "    return -1 * (surr_predicts * np.log(model_preds)).sum(axis=1)\n",
    "\n",
    "def random_sample_loss(surr_model_preds, model_preds):\n",
    "    average_loss = np.ones((model_preds.shape[1]), dtype = np.single)\n",
    "    return average_loss\n",
    "\n",
    "def get_pred_class_prob_after_matching(inputs, ego_matched_lists, ego_model_index):\n",
    "    # if the ensemble model has no prediction matched the ego model, which means it predicts the class is background (0) with probability 1\n",
    "    # output is [objects_num, surr_model_nums , class_nums]\n",
    "    ego_pred_nums = inputs[ego_model_index][\"pred_logits\"].shape[1]\n",
    "    surr_model_nums = len(inputs)\n",
    "    class_nums = inputs[ego_model_index][\"pred_logits\"].shape[2]\n",
    "    background_prob = np.zeros((class_nums), dtype = np.single)\n",
    "    background_prob[0] = 1\n",
    "    out_pred_logits = np.zeros((ego_pred_nums, surr_model_nums , class_nums))\n",
    "    for pred_idx in range(ego_pred_nums):\n",
    "        for i in range(surr_model_nums):\n",
    "            if ego_matched_lists[pred_idx][i] != -1:\n",
    "                temp = inputs[i][\"pred_logits\"][0][ego_matched_lists[pred_idx][i]].sigmoid().numpy()\n",
    "                out_pred_logits[pred_idx, i] = temp / temp.sum()\n",
    "            else:\n",
    "                out_pred_logits[pred_idx, i] = np.copy(background_prob)\n",
    "    return out_pred_logits\n",
    "\n",
    "def get_acquistion_expected_loss(pro_results, model_nums, ego_model_index, loss_type = \"min\", get_loss = entropy_loss):\n",
    "    losses = np.zeros(len(pro_results[0]))\n",
    "    for img_idx in range(len(pro_results[0])):\n",
    "        if pro_results[ego_model_index][img_idx][\"pred_logits\"] is None:\n",
    "            losses[img_idx] = -1000 # template set -1000\n",
    "            continue\n",
    "        inputs = [pro_results[i][img_idx] for i in range(model_nums)]\n",
    "        ego_pred_nums = inputs[ego_model_index][\"pred_logits\"].shape[1]\n",
    "        ego_matched_lists = -np.ones((ego_pred_nums, model_nums), dtype=int)\n",
    "        ego_matched_lists[:, ego_model_index] = np.arange(ego_pred_nums)\n",
    "        for agent in range(model_nums):\n",
    "            if agent == ego_model_index:\n",
    "                continue\n",
    "            match_indices = hungarian_matching(inputs[agent], inputs[ego_model_index])\n",
    "            if len(match_indices) == 0 or len(match_indices[0]) == 0:\n",
    "                continue\n",
    "            ego_matched_lists[match_indices[1], agent] = match_indices[0]\n",
    "\n",
    "        inputs = get_pred_class_prob_after_matching(inputs, ego_matched_lists, ego_model_index)\n",
    "        loss = get_loss(inputs, inputs[:, ego_model_index])\n",
    "        if loss_type == \"min\":\n",
    "            losses[img_idx] = loss.min()\n",
    "        elif loss_type == \"max\":\n",
    "            losses[img_idx] = loss.max()\n",
    "        elif loss_type == \"mean\":\n",
    "            losses[img_idx] = loss.mean()\n",
    "        else:\n",
    "            assert False\n",
    "    # set the loss to be the maximum when prediction of ego model is None\n",
    "    selected_mask = losses == -1000\n",
    "    losses[selected_mask] = losses.max()\n",
    "    return losses\n",
    "\n",
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e771ea32-71ec-4372-973a-2ef8bd27a205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pick_sample_idxs, weights = acquire(expected_losses, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bead5f-6207-450b-8a18-42ab0edcefc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get True Loss for each samples, computation efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6001a0a7-64b3-4761-b50f-479d2429e093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model, criterion, postprocessors = build_model_main(args)\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "_ = model.eval()\n",
    "_ = criterion.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28bde3b0-4b49-4262-b97f-fd2c46eecc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator():\n",
    "    samples_num = len(deep_ensemble_results[ego_model_index])\n",
    "    true_losses = np.zeros((samples_num))\n",
    "    device = torch.device(args.device)\n",
    "    for img_idx in range(samples_num):\n",
    "        imgs, targets = dataset_val[img_idx]\n",
    "        targets = [{k: to_device(v, device) for k, v in targets.items()}]\n",
    "        outputs = {k: to_device(torch.from_numpy(v), device) for k, v in deep_ensemble_results[ego_model_index][img_idx].items()}\n",
    "        outputs['dn_meta'] = None\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        loss=sum(loss_dict_reduced_scaled.values())\n",
    "        true_losses[img_idx] = loss.cpu().numpy()\n",
    "\n",
    "    return true_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72e3a740-08cd-45f2-b527-cdaa68d9ead7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = get_whole_data_set_risk_estimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900ebbc-d8a6-4efb-a198-57bb3ec1211d",
   "metadata": {},
   "source": [
    "## surrogate deep ensemble loss risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3394f8c-e375-4f32-9205-4144fc4a0c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not used for loss calculation efficiency\n",
    "# import random\n",
    "# def run_one_surr_ensemble_risk_estimator(expected_losses, seed, samples_num):\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "#     risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "#     true_losses = np.zeros((samples_num))\n",
    "#     device = torch.device(args.device)\n",
    "#     for i in range(samples_num):\n",
    "#         img_idx = pick_sample_idxs[i]\n",
    "#         imgs, targets = dataset_val[img_idx]\n",
    "#         targets = [{k: to_device(v, device) for k, v in targets.items()}]\n",
    "#         outputs = {k: to_device(torch.from_numpy(v), device) for k, v in deep_ensemble_results[ego_model_index][img_idx].items()}\n",
    "#         outputs['dn_meta'] = None\n",
    "#         loss_dict = criterion(outputs, targets)\n",
    "#         weight_dict = criterion.weight_dict\n",
    "#         # reduce losses over all GPUs for logging purposes\n",
    "#         loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "#         loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "#                                     for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "#         loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "#                                       for k, v in loss_dict_reduced.items()}\n",
    "#         loss=sum(loss_dict_reduced_scaled.values())\n",
    "#         true_losses[i] = loss.cpu().numpy()\n",
    "\n",
    "#     loss_risk = (true_losses * risk_estimator_weights).mean()\n",
    "#     return loss_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a071cb5-40f9-4871-b1db-c77f8aeb6037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_surr_ensemble_risk_estimator(true_losses, expected_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5fbb3cc-74c0-45f1-8a3d-3b3ef2a223fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"surr_ensemble_R50_31_10_runs_min.json\"\n",
    "expected_losses = get_acquistion_expected_loss(pro_results, model_nums, ego_model_index, loss_type = \"min\")\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"surr ensemble\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_surr_ensemble_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5a9512c-7695-4969-921e-faaa7dc89a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"surr_ensemble_R50_31_10_runs_mean.json\"\n",
    "expected_losses = get_acquistion_expected_loss(pro_results, model_nums, ego_model_index, loss_type = \"mean\")\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"surr ensemble\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_surr_ensemble_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b47107-e12b-4ee5-bf33-c004aa69bdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"surr_ensemble_R50_31_10_runs_max.json\"\n",
    "expected_losses = get_acquistion_expected_loss(pro_results, model_nums, ego_model_index, loss_type = \"max\")\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"surr ensemble\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_surr_ensemble_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8d139-8d60-4c6b-98a9-ab306593c9ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce1e5874-bcf8-42f6-a8af-eb0415c571ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d98bd188-991f-450e-a245-e871c91ac503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": len(deep_ensemble_results[ego_model_index])}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(true_losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4399298-445d-496c-af5d-b7fb04a0f475",
   "metadata": {},
   "source": [
    "## Random Sample risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b066bbfb-2ec1-4dcc-a30b-b3082c284bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d286e5d4-befd-4839-a57b-6eb5f78204cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(true_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c481e1-ab87-4537-9e54-cabb7af4bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
