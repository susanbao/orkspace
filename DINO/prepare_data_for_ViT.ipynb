{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d839e79-70ec-420c-b56b-a7c4ab666cbe",
   "metadata": {},
   "source": [
    "## Prepare Data for ViT-based estimation method\n",
    "\n",
    "used code in ViT-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5f5ac9d-00a9-4a5d-90ee-0643ecce2f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe0c52a-4052-4e9c-89c2-a5ff17f3172a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"./data/5_scale_31/\"\n",
    "data_path = base_path + split + \"/data/\"\n",
    "annotation_path = base_path + split + \"/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02bf62d6-e42c-4472-99b8-57baa711e5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "tensor([[[0., 0.],\n",
      "         [1., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.zeros((4,3,2))\n",
    "d[1,0,0] = 1\n",
    "print(d)\n",
    "d = d.permute(1,0,2)\n",
    "print(d)\n",
    "d = d.reshape((d.shape[0], -1))\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b3a175-573b-42a0-abaa-77c232f26d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = \"./data/5_scale_31/train/data/\"\n",
    "test_data_path = \"./data/5_scale_31/val/data/\"\n",
    "train_annotation_path = \"./data/5_scale_31/train/box_annotation/\"\n",
    "test_annotation_path = \"./data/5_scale_31/val/box_annotation/\"\n",
    "train_feature_path = base_path + \"train/feature_data/\"\n",
    "test_feature_path = base_path + \"val/feature_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42ab6b7-bdc8-49e7-b3ff-320351ed6fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def get_numpy_data(data_path, annotation_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        out_results = pred_results[:,selected_index]\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        pred_results = pred_results[:,arrSortedIndex]\n",
    "        one_X = None\n",
    "        for i in range(out_results.shape[1]):\n",
    "            temp = np.append(out_results[:,i], pred_results)\n",
    "            temp = temp.reshape((1,pred_results.shape[1]+1, pred_results.shape[2]))\n",
    "            if one_X is None:\n",
    "                one_X = temp\n",
    "            else:\n",
    "                one_X = np.concatenate((one_X, temp), axis=0)\n",
    "        if X is None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = np.concatenate((X, one_X), axis=0)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb10c5d-1683-4839-bc73-c198f7bd3aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_read(file):\n",
    "    with open(file, \"rb\") as outfile:\n",
    "        data = np.load(outfile)\n",
    "    return data\n",
    "def np_write(data, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        np.save(outfile, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2e9a0b-59d9-4881-855b-7cadf6bbe875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_data(data_path, annotation_path, feature_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        feature = np_read(feature_path + str(img_idx) + \".npy\")\n",
    "        new_feature = np.zeros((1, feature.shape[1], feature.shape[2]*feature.shape[0]))\n",
    "        steps = feature.shape[2]\n",
    "        for i in range(feature.shape[0]):\n",
    "            new_feature[:,:,(steps*i):(steps*i+steps)] = feature[i]\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        query_feature = new_feature[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        query_feature = query_feature[:,arrSortedIndex]\n",
    "        one_X = None\n",
    "        for i in selected_index:\n",
    "            temp = np.concatenate((np.expand_dims(new_feature[:,i], axis=1), query_feature), axis=1)\n",
    "            if one_X is None:\n",
    "                one_X = temp\n",
    "            else:\n",
    "                one_X = np.concatenate((one_X, temp), axis=0)\n",
    "        if X is None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = np.concatenate((X, one_X), axis=0)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    return X, Y\n",
    "\n",
    "# def prepare_feature_data(data_path, annotation_path, feature_path, img_nums, stored_path):\n",
    "#     Y = None\n",
    "#     count = 0\n",
    "#     for img_idx in range(img_nums):\n",
    "#         feature = np_read(feature_path + str(img_idx) + \".npy\")\n",
    "#         new_feature = np.zeros((1, feature.shape[1], feature.shape[2]*feature.shape[0]))\n",
    "#         steps = feature.shape[2]\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             new_feature[:,:,(steps*i):(steps*i+steps)] = feature[i]\n",
    "#         results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "#         pred_logits = np.array(results['input']['pred_logits'])\n",
    "#         pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "#         pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "#         annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "#         selected_index = annotation_data['selected_index']\n",
    "#         loss = annotation_data['loss']\n",
    "#         pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "#         sort_indexs = np.argsort(-pred_logits_max)\n",
    "#         topk_indexs = sort_indexs[:196]\n",
    "#         pred_results = pred_results[:,topk_indexs]\n",
    "#         query_feature = new_feature[:,topk_indexs]\n",
    "#         arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "#         query_feature = query_feature[:,arrSortedIndex]\n",
    "#         for i in selected_index:\n",
    "#             one_X = np.concatenate((new_feature[:,i], query_feature.squeeze(axis=0)), axis=0)\n",
    "#             np_write(one_X, stored_path + str(count) + \".npy\")\n",
    "#             count += 1\n",
    "#         if Y is None:\n",
    "#             Y = loss\n",
    "#         else:\n",
    "#             Y = np.concatenate((Y, loss))\n",
    "#         if img_idx % 100 == 0:\n",
    "#             print(f\"{img_idx} finished\")\n",
    "#     np_write(Y, stored_path + \"annotation.npy\")\n",
    "#     return\n",
    "\n",
    "def prepare_feature_data(data_path, annotation_path, feature_path, img_nums, stored_path):\n",
    "    Y = None\n",
    "    count = 0\n",
    "    for img_idx in range(img_nums):\n",
    "        feature = np_read(feature_path + str(img_idx) + \".npy\")\n",
    "        new_feature = np.zeros((1, feature.shape[1], feature.shape[2]*feature.shape[0]))\n",
    "        steps = feature.shape[2]\n",
    "        for i in range(feature.shape[0]):\n",
    "            new_feature[:,:,(steps*i):(steps*i+steps)] = feature[i]\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        query_feature = new_feature[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        query_feature = query_feature[:,arrSortedIndex].squeeze(axis=0)\n",
    "        np_write(query_feature, stored_path + \"feature\" +str(img_idx) + \".npy\")\n",
    "        for i in selected_index:\n",
    "            one_json = {\"self_feature\": new_feature[:,i].tolist(), \"feature_idx\": img_idx}\n",
    "            write_one_results(stored_path + str(count) + \".json\", one_json)\n",
    "            count += 1\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    np_write(Y, stored_path + \"annotation.npy\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eba719-c073-4b52-ad07-2f19f6ce9e4e",
   "metadata": {},
   "source": [
    "## region-level box ViT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9c1a4-b25c-4551-8627-f612fe254758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X, test_Y = get_numpy_data(test_data_path, test_annotation_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b1317593-0f8a-4091-8a83-72e440a6fe02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3306b35a-48ea-4bc0-978e-c983448164a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49129, 197, 95), (49129,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d4e4a-06b8-42d0-a936-852bbd1c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_numpy_data(train_data_path, train_annotation_path, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f905beb-2b4d-411e-9d1b-8a1fc2746dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, train_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7155c-4d44-42b8-ada3-8928a2888303",
   "metadata": {},
   "source": [
    "## Combine training data from multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be1c815-1b2b-4aa7-b964-822bc4c7c9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "basic_path = f\"./data/5_scale_31/{split}/pre_data/\"\n",
    "# input_files_name = [\"train_box_level_ViT_inputs.npy\", \"train_box_level_ViT_inputs_10000.npy\", \"train_box_level_ViT_inputs_20000.npy\"]\n",
    "# annotations_files_name = [\"train_box_level_ViT_annotations.npy\", \"train_box_level_ViT_annotations_10000.npy\", \"train_box_level_ViT_annotations_20000.npy\"]\n",
    "# output_input_file = \"train_box_level_ViT_inputs_all.npy\"\n",
    "# output_annotations_file = \"train_box_level_ViT_annotations_all.npy\"\n",
    "\n",
    "files_basic_name = f\"{split}_feature_box_level_ViT_\"\n",
    "files_list = [0, 2000, 3000, 4000, 5000]\n",
    "input_files_name = [\"train_box_level_ViT_inputs.npy\", \"train_box_level_ViT_inputs_10000.npy\", \"train_box_level_ViT_inputs_20000.npy\"]\n",
    "annotations_files_name = [\"train_box_level_ViT_annotations.npy\", \"train_box_level_ViT_annotations_10000.npy\", \"train_box_level_ViT_annotations_20000.npy\"]\n",
    "output_input_file = f\"{split}_feature_box_level_ViT_inputs.npy\"\n",
    "output_annotations_file = f\"{split}_feature_box_level_ViT_annotations.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7777330d-8fce-4d3e-910d-b2a6636db7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_all = None\n",
    "annotations_all = None\n",
    "for idx in range(2):\n",
    "    input_file = basic_path + files_basic_name + \"inputs_\" + str(files_list[idx]) + \".npy\"\n",
    "    annotations_file = basic_path + files_basic_name + \"annotations_\" + str(files_list[idx]) + \".npy\"\n",
    "    inputs = np_read(input_file)\n",
    "    annotations = np_read(annotations_file)\n",
    "    if inputs_all is None:\n",
    "        inputs_all = inputs\n",
    "        annotations_all = annotations\n",
    "    else:\n",
    "        inputs_all = np.concatenate((inputs_all, inputs), axis=0)\n",
    "        annotations_all = np.concatenate((annotations_all, annotations), axis=0)\n",
    "np_write(inputs_all, basic_path + output_input_file)\n",
    "np_write(annotations_all, basic_path + output_annotations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ca01c-1021-4634-b0b0-3d56f53c99f7",
   "metadata": {},
   "source": [
    "## Region-level feature ViT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9f99c-d8ae-443a-a2be-84c8fd3527b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X, test_Y = get_feature_data(test_data_path, test_annotation_path, test_feature_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076752f-1e50-41c2-8f35-416d3e0d0b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_feature_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_feature_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd4d26-2e7f-4832-9aef-2fe1472deb2e",
   "metadata": {},
   "source": [
    "## Region-level feature ViT data\n",
    "split the feature into different files so that meta model can load\n",
    "\n",
    "The annotation is still in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df327b62-1c5a-48b2-ac2d-39c40b699c24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 finished\n"
     ]
    }
   ],
   "source": [
    "split = \"val\"\n",
    "stored_file = base_path + split + \"/feature_pre_data/\"\n",
    "prepare_feature_data(test_data_path, test_annotation_path, test_feature_path, 5, stored_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c1db8-ee19-4b38-99e3-7d4960a56934",
   "metadata": {},
   "source": [
    "## Region-level feature ViT data with queries with the smallest IoU distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93398cee-aa3f-44fb-a21c-371f207ebcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_data(data_path, annotation_path, feature_path, img_nums, stored_path):\n",
    "    Y = None\n",
    "    count = 0\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits']).squeeze()\n",
    "        pred_boxes = np.array(results['input']['pred_boxes']).squeeze()\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        loss = annotation_data['loss']\n",
    "        tgt_logits = pred_logits[selected_index]\n",
    "        tgt_bbox = pred_boxes[selected_index]\n",
    "        cost_matrix = hungarian_matching(pred_logits, pred_boxes, tgt_logits, tgt_bbox)\n",
    "        topk_idx = np.argsort(cost_matrix, axis=1)[:, :196]\n",
    "        self_index = np.expand_dims(np.array(selected_index), axis=1)\n",
    "        final_index = np.concatenate((self_index, topk_idx), axis=1)\n",
    "        for i in range(len(selected_index)):\n",
    "            one_json = {\"selected_idxs\": final_index[i].tolist(), \"feature_idx\": img_idx}\n",
    "            write_one_results(stored_path + str(count) + \".json\", one_json)\n",
    "            count += 1\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    np_write(Y, stored_path + \"annotation.npy\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39ec9a7f-c817-4500-a5a5-20bde1c5cdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "def hungarian_matching(out_logits, out_bbox, tgt_logits, tgt_bbox, cost_class = 2.0, cost_bbox = 5.0, cost_giou = 2.0, focal_alpha = 0.25, cost_threshold = 2):\n",
    "    \"\"\" Performs the matching\n",
    "    Params:\n",
    "        outputs/targets: This is a dict that contains at least these entries:\n",
    "             \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits, batch_size = 1\n",
    "             \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "             \"lables\": Tensor of dim [num_queries] with the label of each predicted box\n",
    "        cost_threshold: threshold for distance between outputs and targets\n",
    "    Returns:\n",
    "        cost_matrix\n",
    "    \"\"\"\n",
    "    num_queries = out_logits.shape[0]\n",
    " \n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    out_prob = torch.from_numpy(out_logits).sigmoid()  # [batch_size * num_queries, num_classes]\n",
    "    out_bbox = torch.from_numpy(out_bbox)  # [batch_size * num_queries, 4]\n",
    "    \n",
    "    tgt_ids = np.argmax(tgt_logits, axis=1)\n",
    "    tgt_bbox = torch.from_numpy(tgt_bbox)\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox + cost_class * cost_class + cost_giou * cost_giou\n",
    "    # C = C.view(num_queries, -1)\n",
    "    C = C.numpy()\n",
    "    C = C.T\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "339e812e-8b48-48ad-829d-0b75ec6382ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = train_data_path\n",
    "annotation_path = train_annotation_path\n",
    "feature_path = train_feature_path\n",
    "img_idx = 0\n",
    "results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "pred_logits = np.array(results['input']['pred_logits']).squeeze()\n",
    "pred_boxes = np.array(results['input']['pred_boxes']).squeeze()\n",
    "annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "selected_index = annotation_data['selected_index']\n",
    "loss = annotation_data['loss']\n",
    "tgt_logits = pred_logits[selected_index]\n",
    "tgt_bbox = pred_boxes[selected_index]\n",
    "cost_matrix = hungarian_matching(pred_logits, pred_boxes, tgt_logits, tgt_bbox)\n",
    "topk_idx = np.argsort(cost_matrix, axis=1)[:, :196]\n",
    "self_index = np.expand_dims(np.array(selected_index), axis=1)\n",
    "final_index = np.concatenate((self_index, topk_idx), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f292fca9-ae4f-4e66-98b4-86633a974442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 197)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9e4d9d1-1218-426d-a7f9-3ee65f32b757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a225ba83-a896-4eab-a85b-af1d81423d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d1639b9-16ce-4c1a-a593-909a5bce7b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 197)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa2876-2f6d-452f-9cb8-a8206bd9122d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
