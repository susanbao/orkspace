{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d839e79-70ec-420c-b56b-a7c4ab666cbe",
   "metadata": {},
   "source": [
    "## Prepare Data for ViT-based estimation method\n",
    "\n",
    "used code in ViT-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f5ac9d-00a9-4a5d-90ee-0643ecce2f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe0c52a-4052-4e9c-89c2-a5ff17f3172a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"./data/5_scale_31/\"\n",
    "data_path = base_path + split + \"/data/\"\n",
    "annotation_path = base_path + split + \"/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42ab6b7-bdc8-49e7-b3ff-320351ed6fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def get_numpy_data(data_path, annotation_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        out_results = pred_results[:,selected_index]\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        pred_results = pred_results[:,arrSortedIndex]\n",
    "        one_X = None\n",
    "        for i in range(out_results.shape[1]):\n",
    "            temp = np.append(out_results[:,i], pred_results)\n",
    "            temp = temp.reshape((1,pred_results.shape[1]+1, pred_results.shape[2]))\n",
    "            if one_X is None:\n",
    "                one_X = temp\n",
    "            else:\n",
    "                one_X = np.concatenate((one_X, temp), axis=0)\n",
    "        if X is None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = np.concatenate((X, one_X), axis=0)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5b3a175-573b-42a0-abaa-77c232f26d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = \"./data/5_scale_31/train/data/\"\n",
    "test_data_path = \"./data/5_scale_31/val/data/\"\n",
    "train_annotation_path = \"./data/5_scale_31/train/box_annotation/\"\n",
    "test_annotation_path = \"./data/5_scale_31/val/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9c1a4-b25c-4551-8627-f612fe254758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X, test_Y = get_numpy_data(test_data_path, test_annotation_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b1317593-0f8a-4091-8a83-72e440a6fe02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3306b35a-48ea-4bc0-978e-c983448164a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49129, 197, 95), (49129,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d4e4a-06b8-42d0-a936-852bbd1c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_numpy_data(train_data_path, train_annotation_path, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f905beb-2b4d-411e-9d1b-8a1fc2746dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, train_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3acb4f-ac59-4dbf-aca8-f08b76834fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b7155c-4d44-42b8-ada3-8928a2888303",
   "metadata": {},
   "source": [
    "## Combine training data from multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be1c815-1b2b-4aa7-b964-822bc4c7c9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basic_path = \"./data/5_scale_31/train/pre_data/\"\n",
    "input_files_name = [\"train_box_level_ViT_inputs.npy\", \"train_box_level_ViT_inputs_10000.npy\", \"train_box_level_ViT_inputs_20000.npy\"]\n",
    "annotations_files_name = [\"train_box_level_ViT_annotations.npy\", \"train_box_level_ViT_annotations_10000.npy\", \"train_box_level_ViT_annotations_20000.npy\"]\n",
    "output_input_file = \"train_box_level_ViT_inputs_all.npy\"\n",
    "output_annotations_file = \"train_box_level_ViT_annotations_all.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb10c5d-1683-4839-bc73-c198f7bd3aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_read(file):\n",
    "    with open(file, \"rb\") as outfile:\n",
    "        data = np.load(outfile)\n",
    "    return data\n",
    "def np_write(data, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        np.save(outfile, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7777330d-8fce-4d3e-910d-b2a6636db7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_all = None\n",
    "annotations_all = None\n",
    "for idx in range(2):\n",
    "    input_file = basic_path + input_files_name[idx]\n",
    "    annotations_file = basic_path + annotations_files_name[idx]\n",
    "    inputs = np_read(input_file)\n",
    "    annotations = np_read(annotations_file)\n",
    "    if inputs_all is None:\n",
    "        inputs_all = inputs\n",
    "        annotations_all = annotations\n",
    "    else:\n",
    "        inputs_all = np.concatenate((inputs_all, inputs), axis=0)\n",
    "        annotations_all = np.concatenate((annotations_all, annotations), axis=0)\n",
    "np_write(inputs_all, basic_path + output_input_file)\n",
    "np_write(annotations_all, basic_path + output_annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76552d8a-e42b-43e7-b5fa-c913782a07f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
