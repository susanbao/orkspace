{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d839e79-70ec-420c-b56b-a7c4ab666cbe",
   "metadata": {},
   "source": [
    "## Prepare Data for ViT-based estimation method\n",
    "\n",
    "used code in ViT-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f5ac9d-00a9-4a5d-90ee-0643ecce2f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe0c52a-4052-4e9c-89c2-a5ff17f3172a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"./data/5_scale_31/\"\n",
    "data_path = base_path + split + \"/data/\"\n",
    "annotation_path = base_path + split + \"/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b3a175-573b-42a0-abaa-77c232f26d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = \"./data/5_scale_31/train/data/\"\n",
    "test_data_path = \"./data/5_scale_31/val/data/\"\n",
    "train_annotation_path = \"./data/5_scale_31/train/box_annotation/\"\n",
    "test_annotation_path = \"./data/5_scale_31/val/box_annotation/\"\n",
    "train_feature_path = base_path + \"train/feature_data/\"\n",
    "test_feature_path = base_path + \"val/feature_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42ab6b7-bdc8-49e7-b3ff-320351ed6fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def get_numpy_data(data_path, annotation_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        out_results = pred_results[:,selected_index]\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        pred_results = pred_results[:,arrSortedIndex]\n",
    "        one_X = None\n",
    "        for i in range(out_results.shape[1]):\n",
    "            temp = np.append(out_results[:,i], pred_results)\n",
    "            temp = temp.reshape((1,pred_results.shape[1]+1, pred_results.shape[2]))\n",
    "            if one_X is None:\n",
    "                one_X = temp\n",
    "            else:\n",
    "                one_X = np.concatenate((one_X, temp), axis=0)\n",
    "        if X is None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = np.concatenate((X, one_X), axis=0)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb10c5d-1683-4839-bc73-c198f7bd3aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_read(file):\n",
    "    with open(file, \"rb\") as outfile:\n",
    "        data = np.load(outfile)\n",
    "    return data\n",
    "def np_write(data, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        np.save(outfile, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb2e9a0b-59d9-4881-855b-7cadf6bbe875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_data(data_path, annotation_path, feature_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        feature = np_read(feature_path + str(img_idx) + \".npy\")\n",
    "        new_feature = np.zeros((1, feature.shape[1], feature.shape[2]*feature.shape[0]))\n",
    "        steps = feature.shape[2]\n",
    "        for i in range(feature.shape[0]):\n",
    "            new_feature[:,:,(steps*i):(steps*i+steps)] = feature[i]\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        query_feature = new_feature[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        query_feature = query_feature[:,arrSortedIndex]\n",
    "        one_X = None\n",
    "        for i in selected_index:\n",
    "            temp = np.concatenate((np.expand_dims(new_feature[:,i], axis=1), query_feature), axis=1)\n",
    "            if one_X is None:\n",
    "                one_X = temp\n",
    "            else:\n",
    "                one_X = np.concatenate((one_X, temp), axis=0)\n",
    "        if X is None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = np.concatenate((X, one_X), axis=0)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    return X, Y\n",
    "\n",
    "# def prepare_feature_data(data_path, annotation_path, feature_path, img_nums, stored_path):\n",
    "#     Y = None\n",
    "#     count = 0\n",
    "#     for img_idx in range(img_nums):\n",
    "#         feature = np_read(feature_path + str(img_idx) + \".npy\")\n",
    "#         new_feature = np.zeros((1, feature.shape[1], feature.shape[2]*feature.shape[0]))\n",
    "#         steps = feature.shape[2]\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             new_feature[:,:,(steps*i):(steps*i+steps)] = feature[i]\n",
    "#         results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "#         pred_logits = np.array(results['input']['pred_logits'])\n",
    "#         pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "#         pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "#         annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "#         selected_index = annotation_data['selected_index']\n",
    "#         loss = annotation_data['loss']\n",
    "#         pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "#         sort_indexs = np.argsort(-pred_logits_max)\n",
    "#         topk_indexs = sort_indexs[:196]\n",
    "#         pred_results = pred_results[:,topk_indexs]\n",
    "#         query_feature = new_feature[:,topk_indexs]\n",
    "#         arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "#         query_feature = query_feature[:,arrSortedIndex]\n",
    "#         for i in selected_index:\n",
    "#             one_X = np.concatenate((new_feature[:,i], query_feature.squeeze(axis=0)), axis=0)\n",
    "#             np_write(one_X, stored_path + str(count) + \".npy\")\n",
    "#             count += 1\n",
    "#         if Y is None:\n",
    "#             Y = loss\n",
    "#         else:\n",
    "#             Y = np.concatenate((Y, loss))\n",
    "#         if img_idx % 100 == 0:\n",
    "#             print(f\"{img_idx} finished\")\n",
    "#     np_write(Y, stored_path + \"annotation.npy\")\n",
    "#     return\n",
    "\n",
    "def prepare_feature_data(data_path, annotation_path, feature_path, img_nums, stored_path):\n",
    "    Y = None\n",
    "    count = 0\n",
    "    for img_idx in range(img_nums):\n",
    "        feature = np_read(feature_path + str(img_idx) + \".npy\")\n",
    "        new_feature = np.zeros((1, feature.shape[1], feature.shape[2]*feature.shape[0]))\n",
    "        steps = feature.shape[2]\n",
    "        for i in range(feature.shape[0]):\n",
    "            new_feature[:,:,(steps*i):(steps*i+steps)] = feature[i]\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = np.array(results['input']['pred_logits'])\n",
    "        pred_boxes = np.array(results['input']['pred_boxes'])\n",
    "        pred_results = np.concatenate((pred_boxes, pred_logits), axis=2)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        loss = annotation_data['loss']\n",
    "        pred_logits_max = np.max(pred_logits, axis=2).squeeze()\n",
    "        sort_indexs = np.argsort(-pred_logits_max)\n",
    "        topk_indexs = sort_indexs[:196]\n",
    "        pred_results = pred_results[:,topk_indexs]\n",
    "        query_feature = new_feature[:,topk_indexs]\n",
    "        arrSortedIndex  = np.lexsort((pred_results[:,:,0], pred_results[:,:,1])).squeeze()\n",
    "        query_feature = query_feature[:,arrSortedIndex].squeeze(axis=0)\n",
    "        np_write(query_feature, stored_path + \"feature\" +str(img_idx) + \".npy\")\n",
    "        for i in selected_index:\n",
    "            one_json = {\"self_feature\": new_feature[:,i].tolist(), \"feature_idx\": img_idx}\n",
    "            write_one_results(stored_path + str(count) + \".json\", one_json)\n",
    "            count += 1\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    np_write(Y, stored_path + \"annotation.npy\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eba719-c073-4b52-ad07-2f19f6ce9e4e",
   "metadata": {},
   "source": [
    "## region-level box ViT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9c1a4-b25c-4551-8627-f612fe254758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X, test_Y = get_numpy_data(test_data_path, test_annotation_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b1317593-0f8a-4091-8a83-72e440a6fe02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3306b35a-48ea-4bc0-978e-c983448164a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49129, 197, 95), (49129,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d4e4a-06b8-42d0-a936-852bbd1c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_numpy_data(train_data_path, train_annotation_path, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f905beb-2b4d-411e-9d1b-8a1fc2746dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, train_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7155c-4d44-42b8-ada3-8928a2888303",
   "metadata": {},
   "source": [
    "## Combine training data from multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be1c815-1b2b-4aa7-b964-822bc4c7c9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "basic_path = f\"./data/5_scale_31/{split}/pre_data/\"\n",
    "# input_files_name = [\"train_box_level_ViT_inputs.npy\", \"train_box_level_ViT_inputs_10000.npy\", \"train_box_level_ViT_inputs_20000.npy\"]\n",
    "# annotations_files_name = [\"train_box_level_ViT_annotations.npy\", \"train_box_level_ViT_annotations_10000.npy\", \"train_box_level_ViT_annotations_20000.npy\"]\n",
    "# output_input_file = \"train_box_level_ViT_inputs_all.npy\"\n",
    "# output_annotations_file = \"train_box_level_ViT_annotations_all.npy\"\n",
    "\n",
    "files_basic_name = f\"{split}_feature_box_level_ViT_\"\n",
    "files_list = [0, 2000, 3000, 4000, 5000]\n",
    "input_files_name = [\"train_box_level_ViT_inputs.npy\", \"train_box_level_ViT_inputs_10000.npy\", \"train_box_level_ViT_inputs_20000.npy\"]\n",
    "annotations_files_name = [\"train_box_level_ViT_annotations.npy\", \"train_box_level_ViT_annotations_10000.npy\", \"train_box_level_ViT_annotations_20000.npy\"]\n",
    "output_input_file = f\"{split}_feature_box_level_ViT_inputs.npy\"\n",
    "output_annotations_file = f\"{split}_feature_box_level_ViT_annotations.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7777330d-8fce-4d3e-910d-b2a6636db7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_all = None\n",
    "annotations_all = None\n",
    "for idx in range(2):\n",
    "    input_file = basic_path + files_basic_name + \"inputs_\" + str(files_list[idx]) + \".npy\"\n",
    "    annotations_file = basic_path + files_basic_name + \"annotations_\" + str(files_list[idx]) + \".npy\"\n",
    "    inputs = np_read(input_file)\n",
    "    annotations = np_read(annotations_file)\n",
    "    if inputs_all is None:\n",
    "        inputs_all = inputs\n",
    "        annotations_all = annotations\n",
    "    else:\n",
    "        inputs_all = np.concatenate((inputs_all, inputs), axis=0)\n",
    "        annotations_all = np.concatenate((annotations_all, annotations), axis=0)\n",
    "np_write(inputs_all, basic_path + output_input_file)\n",
    "np_write(annotations_all, basic_path + output_annotations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ca01c-1021-4634-b0b0-3d56f53c99f7",
   "metadata": {},
   "source": [
    "## Region-level feature ViT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1473dfd1-1989-4931-ac8d-83f35a9825ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_feature_path = base_path + \"train/feature_data/\"\n",
    "test_feature_path = base_path + \"val/feature_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9f99c-d8ae-443a-a2be-84c8fd3527b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X, test_Y = get_feature_data(test_data_path, test_annotation_path, test_feature_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076752f-1e50-41c2-8f35-416d3e0d0b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "store_preprocess_inputs_path = base_path + split + f\"/pre_data/{split}_feature_box_level_ViT_inputs.npy\"\n",
    "with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_X)\n",
    "store_preprocess_annotations_path = base_path + split + f\"/pre_data/{split}_feature_box_level_ViT_annotations.npy\"\n",
    "with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "    np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd4d26-2e7f-4832-9aef-2fe1472deb2e",
   "metadata": {},
   "source": [
    "## Region-level feature ViT data\n",
    "split the feature into different files so that meta model can load\n",
    "\n",
    "The annotation is still in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df327b62-1c5a-48b2-ac2d-39c40b699c24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 finished\n"
     ]
    }
   ],
   "source": [
    "split = \"val\"\n",
    "stored_file = base_path + split + \"/feature_pre_data/\"\n",
    "prepare_feature_data(test_data_path, test_annotation_path, test_feature_path, 5, stored_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa3d03-12a4-42cd-9559-508cc3aa93ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
