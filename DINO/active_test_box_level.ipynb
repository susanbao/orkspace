{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7236791-4974-4283-a921-346d8d67ba4e",
   "metadata": {},
   "source": [
    "# Active Testing on Box level\n",
    "\n",
    "data prepared from prepare_box_data.ipynb\n",
    "results store in ./results/active_test_box_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5a9609-4df8-4b18-a6d3-520033708c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f86245-2f26-4436-9fe0-373aba39e578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def transform_tensor_to_list(l):\n",
    "    return l.cpu().tolist()\n",
    "\n",
    "def transform_tensors_to_list(l):\n",
    "    if torch.is_tensor(l):\n",
    "        return transform_tensor_to_list(l)\n",
    "    if isinstance(l, list):\n",
    "        r = []\n",
    "        for i in l:\n",
    "            r.append(transform_tensors_to_list(i))\n",
    "        return r\n",
    "    if isinstance(l, dict):\n",
    "        r = {}\n",
    "        for k,v in l.items():\n",
    "            r[k] = transform_tensors_to_list(v)\n",
    "        return r\n",
    "    return l\n",
    "\n",
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def get_one_annotation_values(path, key, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        if data[key] != None:\n",
    "            results.extend(data[key])\n",
    "        else:\n",
    "            # print(f\"{file} don't have this data\")\n",
    "            result = [-1]*len(data['loss'])\n",
    "            results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def get_img_idxes(path, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        result = [img_idx] * len(data['loss'])\n",
    "        results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        # cur_idx = np.random.randint(expected_loss.size)\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055f10b6-aaf3-4a99-afbc-5afdce865a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "data_path = \"./data/5_scale_31/\" + split + \"/data/\"\n",
    "annotation_path = \"./data/5_scale_31/\" + split + \"/box_annotation/\"\n",
    "result_json_path = \"./results/active_test_box_level_pre/\"\n",
    "box_labels_nums = 36335\n",
    "sample_size_precentage = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "sample_size_set = (np.array(sample_size_precentage) * box_labels_nums).astype(int).tolist()\n",
    "# sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "# simple_size_precent = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f559749-71f5-4407-aa9f-14b2690bbef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = get_one_annotation_values(annotation_path, \"loss\")\n",
    "label_idxes = get_one_annotation_values(annotation_path, \"matched_target_indexes\")\n",
    "img_idxes = get_img_idxes(annotation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3469a3-b1a0-458a-b7d2-777ae1b9c83e",
   "metadata": {},
   "source": [
    "## Random Sample risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4582e716-5d98-4114-bb0f-9c5a30240562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9ea1873-e413-47ad-ae83-4285a9f295fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label might be repeated\n",
    "file_path = result_json_path + \"random_sample_repeated_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample repeated\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc09736-b0cb-4e24-9324-32e5a53c04f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_without_repeated_risk_estimator(true_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    assert true_losses.size == label_idxes.size\n",
    "    idx_array = np.arange(true_losses.size)\n",
    "    count = 0\n",
    "    choose_loss = []\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        idx = np.random.randint(idx_array.size, size=1)\n",
    "        real_idx = idx_array[idx]\n",
    "        choose_loss.extend(true_losses[real_idx].tolist())\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        selected_masked = np.ones(idx_array.shape, dtype=bool)\n",
    "        selected_masked[idx] = False\n",
    "        idx_array = idx_array[selected_masked]\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(choose_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1027c5c6-9420-4c60-8844-184df08cef09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label won't be repeated\n",
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_without_repeated_risk_estimator(losses, img_idxes, label_idxes, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52e3c7-20ec-4590-9728-3e8cde999b2e",
   "metadata": {},
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e41f4bb-e44d-4b31-8a38-67d2bc500d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fe374ff-d70c-4dce-83e1-187514a721f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": losses.size}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee256a-10f2-452f-9be0-f33c3f33acaf",
   "metadata": {},
   "source": [
    "## Try using sklearn model to estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9cca712-e96d-4309-8e47-d03731681212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import copy\n",
    "import sklearn\n",
    "import random\n",
    "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95e5037-0787-40fb-b196-3e091ca407a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(out_logits, out_boxes, tgt_ids, tgt_bbox, cost_class = 2.0, cost_bbox = 5.0, cost_giou = 2.0, focal_alpha = 0.25):\n",
    "    \"\"\" Performs the matching\n",
    "    \"\"\"\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    num_queries = out_logits.shape[0]\n",
    "    out_prob = out_logits.sigmoid()  # [num_queries, num_classes]\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_boxes, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_boxes), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox + cost_class * cost_class + cost_giou * cost_giou\n",
    "    C = C.view(num_queries, -1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c36fc21-15f9-4d0d-9242-dda2edaa2ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_path, annotation_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = torch.FloatTensor(results['input']['pred_logits']).squeeze(axis=0)\n",
    "        pred_boxes = torch.FloatTensor(results['input']['pred_boxes']).squeeze(axis=0)\n",
    "        prob = pred_logits.sigmoid()\n",
    "        labels = pred_logits.argmax(axis=1)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        out_logits = pred_logits[selected_index]\n",
    "        out_boxes = pred_boxes[selected_index]\n",
    "        cost_matrix = hungarian_matching(out_logits, out_boxes, labels, pred_boxes)\n",
    "        topk_values, topk_indexes = torch.topk(-cost_matrix, 9, dim=1)\n",
    "        one_X = None\n",
    "        for i in range(topk_indexes.shape[0]):\n",
    "            surrgate_logits = pred_logits[topk_indexes[i]]\n",
    "            surrgate_boxes = pred_boxes[topk_indexes[i]]\n",
    "            surrgate_data = torch.cat((surrgate_logits, surrgate_boxes), axis=1)\n",
    "            self_data = torch.cat((out_logits[i], out_boxes[i])).unsqueeze(0)\n",
    "            one_temp_X = torch.cat((self_data, surrgate_data), axis=0)\n",
    "            one_temp_X = one_temp_X.reshape((1,-1))\n",
    "            if one_X == None:\n",
    "                one_X = one_temp_X\n",
    "            else:\n",
    "                one_X = torch.cat((one_X, one_temp_X), axis=0)\n",
    "        if X == None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = torch.cat((X, one_X), axis=0)\n",
    "        loss = annotation_data['loss']\n",
    "        loss = np.array(loss)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    X = X.numpy()\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_expected_loss(inputs, model):\n",
    "    expected_loss = model.predict(inputs)\n",
    "    return expected_loss\n",
    "\n",
    "def run_one_approximate_risk_estimator(true_losses, expected_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk\n",
    "\n",
    "def acquire_without_repeated(expected_loss_inputs, img_idxes, label_idxes, samples_num):\n",
    "    assert expected_loss_inputs.size == img_idxes.size\n",
    "    assert expected_loss_inputs.size == label_idxes.size\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = []\n",
    "    pick_sample_idxs = []\n",
    "    count = 0\n",
    "    uniform_clip_val = 0.2\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        idx = np.where(sample)[0][0]\n",
    "        real_idx = idx_array[idx]\n",
    "        weights.append(expected_loss[idx])\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        pick_sample_idxs.append(real_idx)\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(pick_sample_idxs), np.array(weights)\n",
    "\n",
    "def run_one_approximate_risk_estimator_without_repeated(true_losses, expected_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire_without_repeated(expected_losses, img_idxes, label_idxes, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d90ac4c-df76-4fa4-9c81-3abb475d27fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_testing(file_path, true_losses, expected_losses, active_test_type):\n",
    "    json_object = {}\n",
    "    for sample_size in sample_size_set:\n",
    "        for seed in random_seed_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            loss_risk = run_one_approximate_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)\n",
    "\n",
    "def active_testing_without_repeated(file_path, true_losses, expected_losses, active_test_type, img_idxes, label_idxes):\n",
    "    json_object = {}\n",
    "    for sample_size in sample_size_set:\n",
    "        for seed in random_seed_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            loss_risk = run_one_approximate_risk_estimator_without_repeated(true_losses, expected_losses, img_idxes, label_idxes, seed, sample_size)\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b780a2-3c15-4c61-97bf-13f970191b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data_path = \"./data/5_scale_31/train/data/\"\n",
    "# test_data_path = \"./data/5_scale_31/val/data/\"\n",
    "# train_annotation_path = \"./data/5_scale_31/train/box_annotation/\"\n",
    "# test_annotation_path = \"./data/5_scale_31/val/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145170a1-ba01-43cc-9032-71fa5c1f71f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_X, train_Y = get_numpy_data(train_data_path, train_annotation_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e07430c9-1829-45c1-8364-4421edbb6054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"train\"\n",
    "# store_preprocess_inputs_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, train_X)\n",
    "# store_preprocess_annotations_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d2759-20ff-4119-ab5f-bdda2ecc1d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_X, test_Y = get_numpy_data(test_data_path, test_annotation_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f16b1964-6cb5-4f9a-9305-26aae2ceb189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"val\"\n",
    "# store_preprocess_inputs_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, test_X)\n",
    "# store_preprocess_annotations_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac61faf-3ea9-4a16-9f69-33341e92fdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./data/5_scale_31/train\" + f\"/pre_data/train_box_level_inputs.npy\", 'rb') as outfile:\n",
    "    train_X = np.load(outfile)\n",
    "train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "with open(\"./data/5_scale_31/train\" + f\"/pre_data/train_box_level_annotations.npy\", 'rb') as outfile:\n",
    "    train_Y = np.load(outfile)\n",
    "with open(\"./data/5_scale_31/val\" + f\"/pre_data/val_box_level_inputs.npy\", 'rb') as outfile:\n",
    "    test_X = np.load(outfile)\n",
    "test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "with open(\"./data/5_scale_31/val\" + f\"/pre_data/val_box_level_annotations.npy\", 'rb') as outfile:\n",
    "    test_Y = np.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca4585-cfc2-4fd8-945e-3778349bcb37",
   "metadata": {},
   "source": [
    "## Redge Regression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df05b72-7c59-4e09-8f4d-78b9c038637e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.001, 0.01 , 0.1  , 1.   ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "redge_reg = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "redge_reg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe99d78-871f-4f21-88a5-5455daa93ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02169827020655135"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redge_reg.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7e2a638-88d5-481e-b013-0a444db7d0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_repeated_R50_31_10_runs.json\"\n",
    "active_testing(file_path, true_losses, expected_losses, \"RR repeated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff8dcee-7f0f-4fd0-865f-6401540f6edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, expected_losses, \"RR\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd6990a-680f-44d2-81a7-8891f4070d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss_analysis_path = \"./results/loss_analysis/\" + \"box_based_regde_regression.json\"\n",
    "# json_object = {\"true loss\": true_losses.tolist(), \"estimated loss\": expected_losses.tolist()}\n",
    "# write_one_results(loss_analysis_path, json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead734bc-eccd-4e79-9601-90b945d7c37f",
   "metadata": {},
   "source": [
    "### Plot Figure and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a040296e-fac3-4824-a4a6-99bf11f5c1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60849f69-93a5-4c60-9fc0-0263ece54329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "loss_error_rate = (true_losses - expected_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52fb8837-9163-469b-992e-1de991b51daa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(loss_error_rate, bins=50, alpha=0.8, label='boxed-based')\n",
    "plt.title('Region-based Estimated Loss Error Distribution of Regde Regression')\n",
    "plt.xlabel('Estimated Loss Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4ef312a-98b2-4f5f-b43c-4b3e558e5c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09817000761821237, 6.945195486839343)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_losses.min(), expected_losses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2515edf-3429-496f-aa78-e73b657a26fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.977026113308966e-05, 9.686885833740234)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_losses.min(), true_losses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef1582-3f8a-41cd-868e-d33b571f10cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
