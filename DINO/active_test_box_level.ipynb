{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7236791-4974-4283-a921-346d8d67ba4e",
   "metadata": {},
   "source": [
    "# Active Testing on Box level\n",
    "\n",
    "data prepared from prepare_box_data.ipynb\n",
    "results store in ./results/active_test_box_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5a9609-4df8-4b18-a6d3-520033708c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f86245-2f26-4436-9fe0-373aba39e578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def transform_tensor_to_list(l):\n",
    "    return l.cpu().tolist()\n",
    "\n",
    "def transform_tensors_to_list(l):\n",
    "    if torch.is_tensor(l):\n",
    "        return transform_tensor_to_list(l)\n",
    "    if isinstance(l, list):\n",
    "        r = []\n",
    "        for i in l:\n",
    "            r.append(transform_tensors_to_list(i))\n",
    "        return r\n",
    "    if isinstance(l, dict):\n",
    "        r = {}\n",
    "        for k,v in l.items():\n",
    "            r[k] = transform_tensors_to_list(v)\n",
    "        return r\n",
    "    return l\n",
    "\n",
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def get_one_annotation_values(path, key, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        if data[key] != None:\n",
    "            results.extend(data[key])\n",
    "        else:\n",
    "            # print(f\"{file} don't have this data\")\n",
    "            result = [-1]*len(data['loss'])\n",
    "            results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def get_img_idxes(path, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        result = [img_idx] * len(data['loss'])\n",
    "        results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        # cur_idx = np.random.randint(expected_loss.size)\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights\n",
    "\n",
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()\n",
    "\n",
    "def run_one_random_sample_without_repeated_risk_estimator(true_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    assert true_losses.size == label_idxes.size\n",
    "    idx_array = np.arange(true_losses.size)\n",
    "    count = 0\n",
    "    choose_loss = []\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        idx = np.random.randint(idx_array.size, size=1)\n",
    "        real_idx = idx_array[idx]\n",
    "        choose_loss.extend(true_losses[real_idx].tolist())\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        selected_masked = np.ones(idx_array.shape, dtype=bool)\n",
    "        selected_masked[idx] = False\n",
    "        idx_array = idx_array[selected_masked]\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(choose_loss).mean()\n",
    "\n",
    "def change_loss_to_image_level(true_losses, img_idxes):\n",
    "    losses = []\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    index = 0\n",
    "    for i in range(5000):\n",
    "        loss = []\n",
    "        while index < img_idxes.size and img_idxes[index] == i:\n",
    "            loss.append(true_losses[index])\n",
    "            index += 1\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "def dataset_label_num_lists(dataset):\n",
    "    label_nums = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        label_nums.append(target['labels'].shape[0])\n",
    "    return label_nums\n",
    "\n",
    "def run_one_random_sample_risk_estimator_for_image_based(true_losses, label_nums, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(len(true_losses))\n",
    "    sampled_true_losses = []\n",
    "    count_label_num = 0\n",
    "    for i in range(perm.shape[0]):\n",
    "        if count_label_num >= samples_num:\n",
    "            break\n",
    "        sampled_true_losses.extend(true_losses[perm[i]])\n",
    "        count_label_num += label_nums[perm[i]]\n",
    "    sampled_true_losses = np.array(sampled_true_losses)\n",
    "    return sampled_true_losses.mean()\n",
    "\n",
    "def np_read(file):\n",
    "    with open(file, \"rb\") as outfile:\n",
    "        data = np.load(outfile)\n",
    "    return data\n",
    "def np_write(data, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        np.save(outfile, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055f10b6-aaf3-4a99-afbc-5afdce865a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"./data/5_scale_31/\" + split\n",
    "data_path = base_path + \"/data/\"\n",
    "annotation_path = base_path + \"/box_annotation/\"\n",
    "result_json_path = \"./results/active_test_box_level_VIT/\"\n",
    "true_losses = np_read(base_path + \"/feature_pre_data/annotation.npy\")\n",
    "# 36335\n",
    "box_labels_nums = true_losses.shape[0]\n",
    "sample_size_precentage = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "sample_size_set = (np.array(sample_size_precentage) * box_labels_nums).astype(int).tolist()\n",
    "# sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "# simple_size_precent = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "# random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]\n",
    "random_seed_set = [4519, 9524, 5901]                      \n",
    "normalize_list = {\"loss_bbox\": [0.09300409561655773, 0.0767726528828114], \"loss_ce\": [0.3522786986406354, 0.7470140154753052], \n",
    "                  \"loss_giou\": [1.1611697194208146, 0.47297514438862676], \"loss\": [3.1396386155214007, 1.4399662609117525]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f559749-71f5-4407-aa9f-14b2690bbef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = get_one_annotation_values(annotation_path, \"loss\")\n",
    "label_idxes = get_one_annotation_values(annotation_path, \"matched_target_indexes\")\n",
    "img_idxes = get_img_idxes(annotation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4618a-bd51-4521-aa99-6c98a63433a8",
   "metadata": {},
   "source": [
    "## Active Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cca712-e96d-4309-8e47-d03731681212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import copy\n",
    "import sklearn\n",
    "import random\n",
    "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c36fc21-15f9-4d0d-9242-dda2edaa2ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(out_logits, out_boxes, tgt_ids, tgt_bbox, cost_class = 2.0, cost_bbox = 5.0, cost_giou = 2.0, focal_alpha = 0.25):\n",
    "    \"\"\" Performs the matching\n",
    "    \"\"\"\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    num_queries = out_logits.shape[0]\n",
    "    out_prob = out_logits.sigmoid()  # [num_queries, num_classes]\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_boxes, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_boxes), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox + cost_class * cost_class + cost_giou * cost_giou\n",
    "    C = C.view(num_queries, -1)\n",
    "    return C\n",
    "\n",
    "def get_numpy_data_for_regression(data_path, annotation_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = torch.FloatTensor(results['input']['pred_logits']).squeeze(axis=0)\n",
    "        pred_boxes = torch.FloatTensor(results['input']['pred_boxes']).squeeze(axis=0)\n",
    "        prob = pred_logits.sigmoid()\n",
    "        labels = pred_logits.argmax(axis=1)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        out_logits = pred_logits[selected_index]\n",
    "        out_boxes = pred_boxes[selected_index]\n",
    "        cost_matrix = hungarian_matching(out_logits, out_boxes, labels, pred_boxes)\n",
    "        topk_values, topk_indexes = torch.topk(-cost_matrix, 9, dim=1)\n",
    "        one_X = None\n",
    "        for i in range(topk_indexes.shape[0]):\n",
    "            surrgate_logits = pred_logits[topk_indexes[i]]\n",
    "            surrgate_boxes = pred_boxes[topk_indexes[i]]\n",
    "            surrgate_data = torch.cat((surrgate_logits, surrgate_boxes), axis=1)\n",
    "            self_data = torch.cat((out_logits[i], out_boxes[i])).unsqueeze(0)\n",
    "            one_temp_X = torch.cat((self_data, surrgate_data), axis=0)\n",
    "            one_temp_X = one_temp_X.reshape((1,-1))\n",
    "            if one_X == None:\n",
    "                one_X = one_temp_X\n",
    "            else:\n",
    "                one_X = torch.cat((one_X, one_temp_X), axis=0)\n",
    "        if X == None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = torch.cat((X, one_X), axis=0)\n",
    "        loss = annotation_data['loss']\n",
    "        loss = np.array(loss)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    X = X.numpy()\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_expected_loss(inputs, model):\n",
    "    expected_loss = model.predict(inputs)\n",
    "    return expected_loss\n",
    "\n",
    "def run_one_approximate_risk_estimator(true_losses, expected_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk\n",
    "\n",
    "def acquire_without_repeated(expected_loss_inputs, img_idxes, label_idxes, samples_num):\n",
    "    assert expected_loss_inputs.size == img_idxes.size\n",
    "    assert expected_loss_inputs.size == label_idxes.size\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = []\n",
    "    pick_sample_idxs = []\n",
    "    count = 0\n",
    "    uniform_clip_val = 0.2\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        idx = np.where(sample)[0][0]\n",
    "        real_idx = idx_array[idx]\n",
    "        weights.append(expected_loss[idx])\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        pick_sample_idxs.append(real_idx)\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(pick_sample_idxs), np.array(weights)\n",
    "\n",
    "def run_one_approximate_risk_estimator_without_repeated(true_losses, expected_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire_without_repeated(expected_losses, img_idxes, label_idxes, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk\n",
    "\n",
    "def active_testing(file_path, true_losses, expected_losses, active_test_type):\n",
    "    json_object = {}\n",
    "    for sample_size in sample_size_set:\n",
    "        for seed in random_seed_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            loss_risk = run_one_approximate_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)\n",
    "\n",
    "def active_testing_without_repeated(file_path, true_losses, expected_losses, active_test_type, img_idxes, label_idxes):\n",
    "    json_object = {}\n",
    "    for sample_size in sample_size_set:\n",
    "        for seed in random_seed_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            loss_risk = run_one_approximate_risk_estimator_without_repeated(true_losses, expected_losses, img_idxes, label_idxes, seed, sample_size)\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12cc70-61de-4cb4-896f-ac7c33490a00",
   "metadata": {},
   "source": [
    "## Vision Transformer based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8074e2-1b98-4d34-981b-a4b1649cac9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train all ViT parameters\n",
    "estimated_loss_path = \"../../ViT-pytorch/output/ViT-feature-det-ordinal_losses_40000.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_runs_40000.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, estimated_loss, \"ViT\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92babcd9-896b-404e-8a3e-a0d1d43eb9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-feature-Nearest-train-60000-scale_losses.json\"\n",
    "# estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "# estimated_loss = estimated_loss * normalize_list['loss'][1] + normalize_list['loss'][0]\n",
    "# file_path = result_json_path + \"ViT_feature_train_scale_R50_31_10_runs.json\"\n",
    "# active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT feature\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3469a3-b1a0-458a-b7d2-777ae1b9c83e",
   "metadata": {},
   "source": [
    "## Random Sample risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4582e716-5d98-4114-bb0f-9c5a30240562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9ea1873-e413-47ad-ae83-4285a9f295fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label might be repeated\n",
    "file_path = result_json_path + \"random_sample_repeated_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample repeated\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(true_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc09736-b0cb-4e24-9324-32e5a53c04f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_without_repeated_risk_estimator(true_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    assert true_losses.size == label_idxes.size\n",
    "    idx_array = np.arange(true_losses.size)\n",
    "    count = 0\n",
    "    choose_loss = []\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        idx = np.random.randint(idx_array.size, size=1)\n",
    "        real_idx = idx_array[idx]\n",
    "        choose_loss.extend(true_losses[real_idx].tolist())\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        selected_masked = np.ones(idx_array.shape, dtype=bool)\n",
    "        selected_masked[idx] = False\n",
    "        idx_array = idx_array[selected_masked]\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(choose_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1027c5c6-9420-4c60-8844-184df08cef09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label won't be repeated\n",
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_without_repeated_risk_estimator(true_losses, img_idxes, label_idxes, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9f7c5-7de2-4c7a-9872-419ac5e09d10",
   "metadata": {},
   "source": [
    "## Random Sample with image level but set the threshold for box level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e51ed7bf-b078-4ce4-9551-7bbe4ff6a01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_config_path = \"config/DINO/DINO_5scale.py\"\n",
    "args = SLConfig.fromfile(model_config_path) \n",
    "args.dataset_file = 'coco'\n",
    "args.coco_path = \"../coco/\" # the path of coco\n",
    "args.fix_size = False\n",
    "\n",
    "dataset_val = build_dataset(image_set=\"val\", args=args)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8586e658-a00d-4a4c-94da-d267cdbcf879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def change_loss_to_image_level(true_losses, img_idxes):\n",
    "    losses = []\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    index = 0\n",
    "    for i in range(5000):\n",
    "        loss = []\n",
    "        while index < img_idxes.size and img_idxes[index] == i:\n",
    "            loss.append(true_losses[index])\n",
    "            index += 1\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "def dataset_label_num_lists(dataset):\n",
    "    label_nums = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        label_nums.append(target['labels'].shape[0])\n",
    "    return label_nums\n",
    "\n",
    "def run_one_random_sample_risk_estimator_for_image_based(true_losses, label_nums, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(len(true_losses))\n",
    "    sampled_true_losses = []\n",
    "    count_label_num = 0\n",
    "    for i in range(perm.shape[0]):\n",
    "        if count_label_num >= samples_num:\n",
    "            break\n",
    "        sampled_true_losses.extend(true_losses[perm[i]])\n",
    "        count_label_num += label_nums[perm[i]]\n",
    "    sampled_true_losses = np.array(sampled_true_losses)\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e517daa8-94af-4ed4-9f6c-1bc8f56239db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_based_losses = change_loss_to_image_level(losses, img_idxes)\n",
    "label_nums = dataset_label_num_lists(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca13a857-a418-4c7a-99ad-2a828906cbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_image_level_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample image level\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator_for_image_based(image_based_losses, label_nums, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52e3c7-20ec-4590-9728-3e8cde999b2e",
   "metadata": {},
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e41f4bb-e44d-4b31-8a38-67d2bc500d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe374ff-d70c-4dce-83e1-187514a721f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": true_losses.size}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(true_losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee256a-10f2-452f-9be0-f33c3f33acaf",
   "metadata": {},
   "source": [
    "## Try using sklearn model to estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b780a2-3c15-4c61-97bf-13f970191b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data_path = \"./data/5_scale_31/train/data/\"\n",
    "# test_data_path = \"./data/5_scale_31/val/data/\"\n",
    "# train_annotation_path = \"./data/5_scale_31/train/box_annotation/\"\n",
    "# test_annotation_path = \"./data/5_scale_31/val/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145170a1-ba01-43cc-9032-71fa5c1f71f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_X, train_Y = get_numpy_data_for_regression(train_data_path, train_annotation_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e07430c9-1829-45c1-8364-4421edbb6054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"train\"\n",
    "# store_preprocess_inputs_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, train_X)\n",
    "# store_preprocess_annotations_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d2759-20ff-4119-ab5f-bdda2ecc1d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_X, test_Y = get_numpy_data_for_regression(test_data_path, test_annotation_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f16b1964-6cb5-4f9a-9305-26aae2ceb189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"val\"\n",
    "# store_preprocess_inputs_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, test_X)\n",
    "# store_preprocess_annotations_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca4585-cfc2-4fd8-945e-3778349bcb37",
   "metadata": {},
   "source": [
    "## Redge Regression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac61faf-3ea9-4a16-9f69-33341e92fdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./data/5_scale_31/train\" + f\"/pre_data/train_box_level_inputs.npy\", 'rb') as outfile:\n",
    "    train_X = np.load(outfile)\n",
    "train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "with open(\"./data/5_scale_31/train\" + f\"/pre_data/train_box_level_annotations.npy\", 'rb') as outfile:\n",
    "    train_Y = np.load(outfile)\n",
    "with open(\"./data/5_scale_31/val\" + f\"/pre_data/val_box_level_inputs.npy\", 'rb') as outfile:\n",
    "    test_X = np.load(outfile)\n",
    "test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "with open(\"./data/5_scale_31/val\" + f\"/pre_data/val_box_level_annotations.npy\", 'rb') as outfile:\n",
    "    test_Y = np.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df05b72-7c59-4e09-8f4d-78b9c038637e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.001, 0.01 , 0.1  , 1.   ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "redge_reg = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "redge_reg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe99d78-871f-4f21-88a5-5455daa93ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02169827020655135"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redge_reg.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7e2a638-88d5-481e-b013-0a444db7d0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_repeated_R50_31_10_runs.json\"\n",
    "active_testing(file_path, true_losses, expected_losses, \"RR repeated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff8dcee-7f0f-4fd0-865f-6401540f6edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, expected_losses, \"RR\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd6990a-680f-44d2-81a7-8891f4070d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss_analysis_path = \"./results/loss_analysis/\" + \"box_based_regde_regression.json\"\n",
    "# json_object = {\"true loss\": true_losses.tolist(), \"estimated loss\": expected_losses.tolist()}\n",
    "# write_one_results(loss_analysis_path, json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8b630-0f1c-471d-9f3f-e7e93f8da18c",
   "metadata": {},
   "source": [
    "## Redge Regression with feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "484a1d5d-4c13-4328-a12a-007927dae1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_json_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def np_read(file):\n",
    "    with open(file, \"rb\") as outfile:\n",
    "        data = np.load(outfile)\n",
    "    return data\n",
    "\n",
    "def np_write(data, file):\n",
    "    with open(file, \"wb\") as outfile:\n",
    "        np.save(outfile, data)\n",
    "\n",
    "def read_feature(path, nums):\n",
    "    one_feature = np.array(read_one_json_results(feature_pre_data+ \"0.json\")[\"self_feature\"])\n",
    "    features = np.zeros((nums, one_feature.shape[1]))\n",
    "    for index in range(nums):\n",
    "        one_feature = np.array(read_one_json_results(feature_pre_data+ str(index)+\".json\")[\"self_feature\"])\n",
    "        features[index] = one_feature\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b35673d-7899-428b-af81-568c572ae820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_pre_data = \"./data/5_scale_31/train/feature_pre_data/\"\n",
    "train_Y = np_read(feature_pre_data + \"annotation.npy\")\n",
    "train_X = read_feature(feature_pre_data, train_Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2a0bf62-8c5d-4770-b42d-907cbca08f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_pre_data = \"./data/5_scale_31/val/feature_pre_data/\"\n",
    "test_Y = np_read(feature_pre_data + \"annotation.npy\")\n",
    "test_X = read_feature(feature_pre_data, test_Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1186904c-a73a-4f47-9da6-f162ebc1b4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.001, 0.01 , 0.1  , 1.   ]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "redge_reg = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "redge_reg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb516a1f-a173-4928-931a-7c08d19bbc74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11180959630535592"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redge_reg.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0796a9e1-6fb6-44c7-b040-156b16dcb9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_feature_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, expected_losses, \"RR\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49af75ab-adb1-4e3e-81d6-4c72983bb3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_write(expected_losses, \"./results/loss_analysis/box_based_ridge_regression_feature.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233645a-7842-465d-820d-151ac8f4fbca",
   "metadata": {},
   "source": [
    "## MLP with feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38854341-b172-4dd0-a96d-b063a1590388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPLNet(\n",
       "  (layer2): Linear(in_features=1536, out_features=1000, bias=True)\n",
       "  (layer3): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (layer4): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (layer5): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm4): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, DistributedSampler, random_split, TensorDataset\n",
    "from torch import nn\n",
    "class MPLNet(nn.Module):\n",
    "    def __init__(self, input_dims = 10000, output_dims = 1, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer2=nn.Linear(input_dims, 1000)\n",
    "        self.layer3=nn.Linear(1000, 100)\n",
    "        self.layer4=nn.Linear(100, 10)\n",
    "        self.layer5=nn.Linear(10, 1)\n",
    "        self.norm2 = nn.LayerNorm(1000)\n",
    "        self.norm3 = nn.LayerNorm(100)\n",
    "        self.norm4 = nn.LayerNorm(10)\n",
    "        self._init_parms(self.layer2)\n",
    "        self._init_parms(self.layer3)\n",
    "        self._init_parms(self.layer4)\n",
    "        self._init_parms(self.layer5)\n",
    "        # self.dropout1 = nn.Dropout(dropout)\n",
    "        # self.dropout2 = nn.Dropout(dropout)\n",
    "        # self.dropout3 = nn.Dropout(dropout)\n",
    "        self.input_dims = input_dims\n",
    "    \n",
    "    def _init_parms(self, module):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, self.input_dims)\n",
    "        x=nn.functional.relu(self.norm2(self.layer2(x)))\n",
    "        x=nn.functional.relu(self.norm3(self.layer3(x)))\n",
    "        x=nn.functional.relu(self.norm4(self.layer4(x)))\n",
    "        x=self.layer5(x)\n",
    "        return x\n",
    "\n",
    "device_name = \"cuda:1\"\n",
    "device = torch.device(device_name)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "rate_learning = 1e-4\n",
    "model = MPLNet(input_dims=1536)\n",
    "loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=rate_learning)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b15a116d-61d5-4b75-9920-5b1411d56c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "batch_size = 100\n",
    "inputs = torch.from_numpy(train_X)\n",
    "annotations = torch.from_numpy(train_Y)\n",
    "datasets = TensorDataset(inputs, annotations)\n",
    "datasets_nums = inputs.shape[0]\n",
    "train_nums = int(datasets_nums * 0.7)\n",
    "train_datasets, val_datasets = torch.utils.data.random_split(datasets, [train_nums, datasets_nums-train_nums])\n",
    "train_sampler = torch.utils.data.RandomSampler(train_datasets)\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(train_sampler, batch_size, drop_last=True)\n",
    "train_dataloader = DataLoader(train_datasets, batch_sampler=batch_sampler_train)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_datasets)\n",
    "valid_dataloader = DataLoader(val_datasets, 1, sampler=val_sampler, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e26cd67c-e068-43dc-a9b6-1eee8aae77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Step: 0, Loss 1.9148993492126465\n",
      "Epoch: 0\n",
      "Training loss: 2.071711402107717\n",
      "Validation loss: 2.0650121677309454\n",
      "Train Epoch: 1, Step: 0, Loss 2.1761560440063477\n",
      "Epoch: 1\n",
      "Training loss: 2.0716718355521513\n",
      "Validation loss: 2.0647357225670593\n",
      "Train Epoch: 2, Step: 0, Loss 2.6170666217803955\n",
      "Epoch: 2\n",
      "Training loss: 2.071611847088636\n",
      "Validation loss: 2.064915754937436\n",
      "Train Epoch: 3, Step: 0, Loss 2.2529234886169434\n",
      "Epoch: 3\n",
      "Training loss: 2.0714700632778684\n",
      "Validation loss: 2.0657523120824597\n",
      "Train Epoch: 4, Step: 0, Loss 2.727835178375244\n",
      "Epoch: 4\n",
      "Training loss: 2.0716633220743956\n",
      "Validation loss: 2.065128861864115\n",
      "Train Epoch: 5, Step: 0, Loss 1.8226053714752197\n",
      "Epoch: 5\n",
      "Training loss: 2.0715278798461085\n",
      "Validation loss: 2.063770113586585\n",
      "Train Epoch: 6, Step: 0, Loss 2.1272594928741455\n",
      "Epoch: 6\n",
      "Training loss: 2.071536196170039\n",
      "Validation loss: 2.0643691218363864\n",
      "Train Epoch: 7, Step: 0, Loss 2.599735975265503\n",
      "Epoch: 7\n",
      "Training loss: 2.0714579938083606\n",
      "Validation loss: 2.0647001738408015\n",
      "Train Epoch: 8, Step: 0, Loss 2.351402997970581\n",
      "Epoch: 8\n",
      "Training loss: 2.0715536476437735\n",
      "Validation loss: 2.0651351861331717\n",
      "Train Epoch: 9, Step: 0, Loss 1.7966591119766235\n",
      "Epoch: 9\n",
      "Training loss: 2.071572874883706\n",
      "Validation loss: 2.0648116522646847\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    # Performing Training for each epoch\n",
    "    training_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    # The training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input, label = batch\n",
    "        input = input.to(device=device_name).type(torch.float)\n",
    "        label = label.to(device=device_name).type(torch.float)[None, :]\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        if step % 10000 == 0:\n",
    "            print(\"Train Epoch: {}, Step: {}, Loss {}\".format(epoch, step, loss.item()))\n",
    "\n",
    "\n",
    "    # Performing Validation for each epoch\n",
    "    validation_loss = 0.\n",
    "    model.eval()\n",
    "\n",
    "    # The validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        input, label = batch\n",
    "        input = input.to(device=device_name).type(torch.float)\n",
    "        label = label.to(device=device_name).type(torch.float)[None, :]\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "    # Calculating the average training and validation loss over epoch\n",
    "    training_loss_avg = training_loss/len(train_dataloader)\n",
    "    validation_loss_avg = validation_loss/len(valid_dataloader)\n",
    "\n",
    "    # Printing average training and average validation losses\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    print(\"Training loss: {}\".format(training_loss_avg))\n",
    "    print(\"Validation loss: {}\".format(validation_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d24f4d3-b75a-4d91-8649-fffde3033a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_loss_MLP(inputs):\n",
    "    model.eval()\n",
    "    datasets_nums = inputs.shape[0]\n",
    "    expected_loss = np.zeros(datasets_nums)\n",
    "    for i in range(datasets_nums):\n",
    "        input = torch.from_numpy(inputs[i]).to(device=device_name).type(torch.float)\n",
    "        output = model(input)\n",
    "        expected_loss[i] = output.detach().cpu().numpy()\n",
    "    return expected_loss\n",
    "\n",
    "expected_losses = get_expected_loss_MLP(test_X)\n",
    "file_path = result_json_path + \"MLP_feature_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, expected_losses, \"MLP\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2fa59ae-ee3a-47a1-89cd-e2f0d3620b58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5110464909313053"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power((expected_losses - true_losses), 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5c67f9f-ffb2-47f5-ba48-1eee4e81edfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def display_data_hist(loss):\n",
    "    plt.hist(loss, bins=50, label='Box-based')\n",
    "    plt.title('Loss Distribution')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Probability Density')\n",
    "    locs, _ = plt.yticks()\n",
    "    plt.yticks(locs,np.round(locs/loss.shape[0],3))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "display_data_hist(expected_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aba4ce-c84e-4029-b773-6b99ee09a532",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6da87c7-6a2a-4a9d-a312-18e64cb7f838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train all ViT parameters\n",
    "estimated_loss_path = \"../../ViT-pytorch/output/ViT-feature-det-ordinal_losses_50000.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_runs_50000.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, estimated_loss, \"ViT\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf5e2168-464b-446b-bf0e-333381a20957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only train part of ViT parameters, freeze transformer part\n",
    "estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-train-with-test_retrain_losses.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_test_retrain_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT Test Retrain\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b9ae0f-d5ae-413b-a574-a33b7dfa7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only train part of ViT parameters, freeze transformer part\n",
    "estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-train-20000_losses.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ec539d-e1a3-4f8c-8dc5-10453ac1fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature ViT\n",
    "estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-feature-train-with-test_losses.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_feature_test_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT feature test\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3695189-987d-42ee-9ef3-e442d4517072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-feature-Nearest-train-60000-without-warmup_losses.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_feature_train_R50_31_10_without_warmup_runs.json\"\n",
    "active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT feature\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e3749-eb82-4c7b-ac72-b4c71863f039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
