{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7236791-4974-4283-a921-346d8d67ba4e",
   "metadata": {},
   "source": [
    "# Active Testing on Box level\n",
    "\n",
    "data prepared from prepare_box_data.ipynb\n",
    "results store in ./results/active_test_box_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5a9609-4df8-4b18-a6d3-520033708c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f86245-2f26-4436-9fe0-373aba39e578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def transform_tensor_to_list(l):\n",
    "    return l.cpu().tolist()\n",
    "\n",
    "def transform_tensors_to_list(l):\n",
    "    if torch.is_tensor(l):\n",
    "        return transform_tensor_to_list(l)\n",
    "    if isinstance(l, list):\n",
    "        r = []\n",
    "        for i in l:\n",
    "            r.append(transform_tensors_to_list(i))\n",
    "        return r\n",
    "    if isinstance(l, dict):\n",
    "        r = {}\n",
    "        for k,v in l.items():\n",
    "            r[k] = transform_tensors_to_list(v)\n",
    "        return r\n",
    "    return l\n",
    "\n",
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def get_one_annotation_values(path, key, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        if data[key] != None:\n",
    "            results.extend(data[key])\n",
    "        else:\n",
    "            # print(f\"{file} don't have this data\")\n",
    "            result = [-1]*len(data['loss'])\n",
    "            results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def get_img_idxes(path, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        result = [img_idx] * len(data['loss'])\n",
    "        results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        # cur_idx = np.random.randint(expected_loss.size)\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055f10b6-aaf3-4a99-afbc-5afdce865a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "data_path = \"./data/5_scale_31/\" + split + \"/data/\"\n",
    "annotation_path = \"./data/5_scale_31/\" + split + \"/box_annotation/\"\n",
    "result_json_path = \"./results/active_test_box_level/\"\n",
    "sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f559749-71f5-4407-aa9f-14b2690bbef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = get_one_annotation_values(annotation_path, \"loss\")\n",
    "label_idxes = get_one_annotation_values(annotation_path, \"matched_target_indexes\")\n",
    "img_idxes = get_img_idxes(annotation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3469a3-b1a0-458a-b7d2-777ae1b9c83e",
   "metadata": {},
   "source": [
    "## Random Sample risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4582e716-5d98-4114-bb0f-9c5a30240562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ea1873-e413-47ad-ae83-4285a9f295fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label might be repeated\n",
    "file_path = result_json_path + \"random_sample_repeated_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample repeated\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc09736-b0cb-4e24-9324-32e5a53c04f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_without_repeated_risk_estimator(true_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    assert true_losses.size == label_idxes.size\n",
    "    idx_array = np.arange(true_losses.size)\n",
    "    count = 0\n",
    "    choose_loss = []\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        idx = np.random.randint(idx_array.size, size=1)\n",
    "        real_idx = idx_array[idx]\n",
    "        choose_loss.extend(true_losses[real_idx].tolist())\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(choose_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1027c5c6-9420-4c60-8844-184df08cef09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label won't be repeated\n",
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_without_repeated_risk_estimator(losses, img_idxes, label_idxes, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52e3c7-20ec-4590-9728-3e8cde999b2e",
   "metadata": {},
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e41f4bb-e44d-4b31-8a38-67d2bc500d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe374ff-d70c-4dce-83e1-187514a721f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": losses.size}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810b288-da5c-4fc1-b97f-20e5de90eebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
