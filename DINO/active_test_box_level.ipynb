{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7236791-4974-4283-a921-346d8d67ba4e",
   "metadata": {},
   "source": [
    "# Active Testing on Box level\n",
    "\n",
    "data prepared from prepare_box_data.ipynb\n",
    "results store in ./results/active_test_box_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5a9609-4df8-4b18-a6d3-520033708c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from util.utils import slprint, to_device\n",
    "import util.misc as utils\n",
    "from engine import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import build_dataset, get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f86245-2f26-4436-9fe0-373aba39e578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_one_image_results(path):\n",
    "    with open(path, \"r\") as outfile:\n",
    "        data = json.load(outfile)\n",
    "    return data\n",
    "\n",
    "def write_one_results(path, json_data):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(json_data, outfile)\n",
    "        \n",
    "def transform_tensor_to_list(l):\n",
    "    return l.cpu().tolist()\n",
    "\n",
    "def transform_tensors_to_list(l):\n",
    "    if torch.is_tensor(l):\n",
    "        return transform_tensor_to_list(l)\n",
    "    if isinstance(l, list):\n",
    "        r = []\n",
    "        for i in l:\n",
    "            r.append(transform_tensors_to_list(i))\n",
    "        return r\n",
    "    if isinstance(l, dict):\n",
    "        r = {}\n",
    "        for k,v in l.items():\n",
    "            r[k] = transform_tensors_to_list(v)\n",
    "        return r\n",
    "    return l\n",
    "\n",
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def get_one_annotation_values(path, key, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        if data[key] != None:\n",
    "            results.extend(data[key])\n",
    "        else:\n",
    "            # print(f\"{file} don't have this data\")\n",
    "            result = [-1]*len(data['loss'])\n",
    "            results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def get_img_idxes(path, file_num = 5000):\n",
    "    results = []\n",
    "    for img_idx in range(file_num):\n",
    "        file_path = os.path.join(path, str(img_idx)+\".json\")\n",
    "        data = read_one_image_results(file_path)\n",
    "        result = [img_idx] * len(data['loss'])\n",
    "        results.extend(result)\n",
    "    return np.array(results)\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        # cur_idx = np.random.randint(expected_loss.size)\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055f10b6-aaf3-4a99-afbc-5afdce865a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "data_path = \"./data/5_scale_31/\" + split + \"/data/\"\n",
    "annotation_path = \"./data/5_scale_31/\" + split + \"/box_annotation/\"\n",
    "result_json_path = \"./results/active_test_box_level_pre/\"\n",
    "box_labels_nums = 36335\n",
    "sample_size_precentage = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "sample_size_set = (np.array(sample_size_precentage) * box_labels_nums).astype(int).tolist()\n",
    "# sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "# simple_size_precent = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f559749-71f5-4407-aa9f-14b2690bbef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = get_one_annotation_values(annotation_path, \"loss\")\n",
    "label_idxes = get_one_annotation_values(annotation_path, \"matched_target_indexes\")\n",
    "img_idxes = get_img_idxes(annotation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3469a3-b1a0-458a-b7d2-777ae1b9c83e",
   "metadata": {},
   "source": [
    "## Random Sample risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4582e716-5d98-4114-bb0f-9c5a30240562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9ea1873-e413-47ad-ae83-4285a9f295fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label might be repeated\n",
    "file_path = result_json_path + \"random_sample_repeated_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample repeated\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc09736-b0cb-4e24-9324-32e5a53c04f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_without_repeated_risk_estimator(true_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    assert true_losses.size == label_idxes.size\n",
    "    idx_array = np.arange(true_losses.size)\n",
    "    count = 0\n",
    "    choose_loss = []\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        idx = np.random.randint(idx_array.size, size=1)\n",
    "        real_idx = idx_array[idx]\n",
    "        choose_loss.extend(true_losses[real_idx].tolist())\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        selected_masked = np.ones(idx_array.shape, dtype=bool)\n",
    "        selected_masked[idx] = False\n",
    "        idx_array = idx_array[selected_masked]\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(choose_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1027c5c6-9420-4c60-8844-184df08cef09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The required label won't be repeated\n",
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_without_repeated_risk_estimator(losses, img_idxes, label_idxes, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9f7c5-7de2-4c7a-9872-419ac5e09d10",
   "metadata": {},
   "source": [
    "## Random Sample with image level but set the threshold for box level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e51ed7bf-b078-4ce4-9551-7bbe4ff6a01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_config_path = \"config/DINO/DINO_5scale.py\"\n",
    "args = SLConfig.fromfile(model_config_path) \n",
    "args.dataset_file = 'coco'\n",
    "args.coco_path = \"../coco/\" # the path of coco\n",
    "args.fix_size = False\n",
    "\n",
    "dataset_val = build_dataset(image_set=\"val\", args=args)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8586e658-a00d-4a4c-94da-d267cdbcf879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def change_loss_to_image_level(true_losses, img_idxes):\n",
    "    losses = []\n",
    "    assert true_losses.size == img_idxes.size\n",
    "    index = 0\n",
    "    for i in range(5000):\n",
    "        loss = []\n",
    "        while index < img_idxes.size and img_idxes[index] == i:\n",
    "            loss.append(true_losses[index])\n",
    "            index += 1\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "def dataset_label_num_lists(dataset):\n",
    "    label_nums = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        label_nums.append(target['labels'].shape[0])\n",
    "    return label_nums\n",
    "\n",
    "def run_one_random_sample_risk_estimator_for_image_based(true_losses, label_nums, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(len(true_losses))\n",
    "    sampled_true_losses = []\n",
    "    count_label_num = 0\n",
    "    for i in range(perm.shape[0]):\n",
    "        if count_label_num >= samples_num:\n",
    "            break\n",
    "        sampled_true_losses.extend(true_losses[perm[i]])\n",
    "        count_label_num += label_nums[perm[i]]\n",
    "    sampled_true_losses = np.array(sampled_true_losses)\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e517daa8-94af-4ed4-9f6c-1bc8f56239db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_based_losses = change_loss_to_image_level(losses, img_idxes)\n",
    "label_nums = dataset_label_num_lists(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca13a857-a418-4c7a-99ad-2a828906cbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_image_level_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample image level\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator_for_image_based(image_based_losses, label_nums, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52e3c7-20ec-4590-9728-3e8cde999b2e",
   "metadata": {},
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e41f4bb-e44d-4b31-8a38-67d2bc500d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fe374ff-d70c-4dce-83e1-187514a721f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": losses.size}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee256a-10f2-452f-9be0-f33c3f33acaf",
   "metadata": {},
   "source": [
    "## Try using sklearn model to estimate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9cca712-e96d-4309-8e47-d03731681212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import copy\n",
    "import sklearn\n",
    "import random\n",
    "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95e5037-0787-40fb-b196-3e091ca407a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hungarian_matching(out_logits, out_boxes, tgt_ids, tgt_bbox, cost_class = 2.0, cost_bbox = 5.0, cost_giou = 2.0, focal_alpha = 0.25):\n",
    "    \"\"\" Performs the matching\n",
    "    \"\"\"\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    num_queries = out_logits.shape[0]\n",
    "    out_prob = out_logits.sigmoid()  # [num_queries, num_classes]\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_boxes, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_boxes), box_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox + cost_class * cost_class + cost_giou * cost_giou\n",
    "    C = C.view(num_queries, -1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c36fc21-15f9-4d0d-9242-dda2edaa2ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_numpy_data_for_regression(data_path, annotation_path, img_nums):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img_idx in range(img_nums):\n",
    "        results = read_one_image_results(data_path + str(img_idx) + \".json\")\n",
    "        pred_logits = torch.FloatTensor(results['input']['pred_logits']).squeeze(axis=0)\n",
    "        pred_boxes = torch.FloatTensor(results['input']['pred_boxes']).squeeze(axis=0)\n",
    "        prob = pred_logits.sigmoid()\n",
    "        labels = pred_logits.argmax(axis=1)\n",
    "        annotation_data = read_one_image_results(annotation_path + str(img_idx) + \".json\")\n",
    "        selected_index = annotation_data['selected_index']\n",
    "        out_logits = pred_logits[selected_index]\n",
    "        out_boxes = pred_boxes[selected_index]\n",
    "        cost_matrix = hungarian_matching(out_logits, out_boxes, labels, pred_boxes)\n",
    "        topk_values, topk_indexes = torch.topk(-cost_matrix, 9, dim=1)\n",
    "        one_X = None\n",
    "        for i in range(topk_indexes.shape[0]):\n",
    "            surrgate_logits = pred_logits[topk_indexes[i]]\n",
    "            surrgate_boxes = pred_boxes[topk_indexes[i]]\n",
    "            surrgate_data = torch.cat((surrgate_logits, surrgate_boxes), axis=1)\n",
    "            self_data = torch.cat((out_logits[i], out_boxes[i])).unsqueeze(0)\n",
    "            one_temp_X = torch.cat((self_data, surrgate_data), axis=0)\n",
    "            one_temp_X = one_temp_X.reshape((1,-1))\n",
    "            if one_X == None:\n",
    "                one_X = one_temp_X\n",
    "            else:\n",
    "                one_X = torch.cat((one_X, one_temp_X), axis=0)\n",
    "        if X == None:\n",
    "            X = one_X\n",
    "        else:\n",
    "            X = torch.cat((X, one_X), axis=0)\n",
    "        loss = annotation_data['loss']\n",
    "        loss = np.array(loss)\n",
    "        if Y is None:\n",
    "            Y = loss\n",
    "        else:\n",
    "            Y = np.concatenate((Y, loss))\n",
    "        if img_idx % 100 == 0:\n",
    "            print(f\"{img_idx} finished\")\n",
    "    X = X.numpy()\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_expected_loss(inputs, model):\n",
    "    expected_loss = model.predict(inputs)\n",
    "    return expected_loss\n",
    "\n",
    "def run_one_approximate_risk_estimator(true_losses, expected_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk\n",
    "\n",
    "def acquire_without_repeated(expected_loss_inputs, img_idxes, label_idxes, samples_num):\n",
    "    assert expected_loss_inputs.size == img_idxes.size\n",
    "    assert expected_loss_inputs.size == label_idxes.size\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = []\n",
    "    pick_sample_idxs = []\n",
    "    count = 0\n",
    "    uniform_clip_val = 0.2\n",
    "    choose_box_idx = {}\n",
    "    while count < samples_num:\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        idx = np.where(sample)[0][0]\n",
    "        real_idx = idx_array[idx]\n",
    "        weights.append(expected_loss[idx])\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "        img_idx = int(img_idxes[real_idx])\n",
    "        lable_idx = int(label_idxes[real_idx])\n",
    "        pick_sample_idxs.append(real_idx)\n",
    "        if img_idx in choose_box_idx:\n",
    "            if lable_idx in choose_box_idx[img_idx]:\n",
    "                continue\n",
    "            choose_box_idx[img_idx].append(lable_idx)\n",
    "        else:\n",
    "            choose_box_idx[img_idx] = [lable_idx]\n",
    "        count += 1\n",
    "    return np.array(pick_sample_idxs), np.array(weights)\n",
    "\n",
    "def run_one_approximate_risk_estimator_without_repeated(true_losses, expected_losses, img_idxes, label_idxes, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire_without_repeated(expected_losses, img_idxes, label_idxes, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d90ac4c-df76-4fa4-9c81-3abb475d27fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_testing(file_path, true_losses, expected_losses, active_test_type):\n",
    "    json_object = {}\n",
    "    for sample_size in sample_size_set:\n",
    "        for seed in random_seed_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            loss_risk = run_one_approximate_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)\n",
    "\n",
    "def active_testing_without_repeated(file_path, true_losses, expected_losses, active_test_type, img_idxes, label_idxes):\n",
    "    json_object = {}\n",
    "    for sample_size in sample_size_set:\n",
    "        for seed in random_seed_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            loss_risk = run_one_approximate_risk_estimator_without_repeated(true_losses, expected_losses, img_idxes, label_idxes, seed, sample_size)\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b780a2-3c15-4c61-97bf-13f970191b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data_path = \"./data/5_scale_31/train/data/\"\n",
    "# test_data_path = \"./data/5_scale_31/val/data/\"\n",
    "# train_annotation_path = \"./data/5_scale_31/train/box_annotation/\"\n",
    "# test_annotation_path = \"./data/5_scale_31/val/box_annotation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145170a1-ba01-43cc-9032-71fa5c1f71f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_X, train_Y = get_numpy_data_for_regression(train_data_path, train_annotation_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e07430c9-1829-45c1-8364-4421edbb6054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"train\"\n",
    "# store_preprocess_inputs_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, train_X)\n",
    "# store_preprocess_annotations_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d2759-20ff-4119-ab5f-bdda2ecc1d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_X, test_Y = get_numpy_data_for_regression(test_data_path, test_annotation_path, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f16b1964-6cb5-4f9a-9305-26aae2ceb189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split = \"val\"\n",
    "# store_preprocess_inputs_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, test_X)\n",
    "# store_preprocess_annotations_path = \"./data/5_scale_31/\" + split + f\"/pre_data/{split}_box_level_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca4585-cfc2-4fd8-945e-3778349bcb37",
   "metadata": {},
   "source": [
    "## Redge Regression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac61faf-3ea9-4a16-9f69-33341e92fdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./data/5_scale_31/train\" + f\"/pre_data/train_box_level_inputs.npy\", 'rb') as outfile:\n",
    "    train_X = np.load(outfile)\n",
    "train_X = train_X.reshape((train_X.shape[0], -1))\n",
    "with open(\"./data/5_scale_31/train\" + f\"/pre_data/train_box_level_annotations.npy\", 'rb') as outfile:\n",
    "    train_Y = np.load(outfile)\n",
    "with open(\"./data/5_scale_31/val\" + f\"/pre_data/val_box_level_inputs.npy\", 'rb') as outfile:\n",
    "    test_X = np.load(outfile)\n",
    "test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "with open(\"./data/5_scale_31/val\" + f\"/pre_data/val_box_level_annotations.npy\", 'rb') as outfile:\n",
    "    test_Y = np.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df05b72-7c59-4e09-8f4d-78b9c038637e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.001, 0.01 , 0.1  , 1.   ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "redge_reg = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n",
    "redge_reg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe99d78-871f-4f21-88a5-5455daa93ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02169827020655135"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redge_reg.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7e2a638-88d5-481e-b013-0a444db7d0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_repeated_R50_31_10_runs.json\"\n",
    "active_testing(file_path, true_losses, expected_losses, \"RR repeated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff8dcee-7f0f-4fd0-865f-6401540f6edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = test_Y\n",
    "expected_losses = get_expected_loss(test_X, redge_reg)\n",
    "file_path = result_json_path + \"RedgeReg_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, true_losses, expected_losses, \"RR\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd6990a-680f-44d2-81a7-8891f4070d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss_analysis_path = \"./results/loss_analysis/\" + \"box_based_regde_regression.json\"\n",
    "# json_object = {\"true loss\": true_losses.tolist(), \"estimated loss\": expected_losses.tolist()}\n",
    "# write_one_results(loss_analysis_path, json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aba4ce-c84e-4029-b773-6b99ee09a532",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6da87c7-6a2a-4a9d-a312-18e64cb7f838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train all ViT parameters\n",
    "estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-train-with-test_losses.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4dbcd00-fb69-48bc-867d-1ec1d142dd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"ViT_test_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT Test\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf5e2168-464b-446b-bf0e-333381a20957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only train part of ViT parameters, freeze transformer part\n",
    "estimated_loss_path = \"/workspace/ViT-pytorch/output/ViT-train-with-test_retrain_losses.json\"\n",
    "estimated_loss = np.array(read_one_image_results(estimated_loss_path)['losses'])\n",
    "file_path = result_json_path + \"ViT_test_retrain_R50_31_10_runs.json\"\n",
    "active_testing_without_repeated(file_path, losses, estimated_loss, \"ViT Test Retrain\", img_idxes, label_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9ae0f-d5ae-413b-a574-a33b7dfa7daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
