{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf872ef-b687-4fc4-a6bb-4c77bdfc6826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler, random_split, TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a12d68-b893-454d-9fdd-f320a3e587de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "model_data_path = \"./data/5_scale_31/\"\n",
    "data_path = model_data_path + split + \"/data/\"\n",
    "summary_annotation_file = model_data_path + split + \"/summary.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1ccf01a-82dc-4ea7-96f7-4dbbcfd652dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pred_logits', 'pred_boxes', 'aux_outputs', 'interm_outputs', 'interm_outputs_for_matching_pre', 'dn_meta'])\n",
      "dict_keys(['loss', 'loss_bbox_dn', 'loss_giou_dn', 'loss_ce_dn', 'loss_ce', 'loss_bbox', 'loss_giou', 'loss_ce_0', 'loss_bbox_0', 'loss_giou_0', 'loss_bbox_dn_0', 'loss_giou_dn_0', 'loss_ce_dn_0', 'loss_ce_1', 'loss_bbox_1', 'loss_giou_1', 'loss_bbox_dn_1', 'loss_giou_dn_1', 'loss_ce_dn_1', 'loss_ce_2', 'loss_bbox_2', 'loss_giou_2', 'loss_bbox_dn_2', 'loss_giou_dn_2', 'loss_ce_dn_2', 'loss_ce_3', 'loss_bbox_3', 'loss_giou_3', 'loss_bbox_dn_3', 'loss_giou_dn_3', 'loss_ce_dn_3', 'loss_ce_4', 'loss_bbox_4', 'loss_giou_4', 'loss_bbox_dn_4', 'loss_giou_dn_4', 'loss_ce_dn_4', 'loss_ce_interm', 'loss_bbox_interm', 'loss_giou_interm', 'loss_bbox_dn_unscaled', 'loss_giou_dn_unscaled', 'loss_ce_dn_unscaled', 'loss_xy_dn_unscaled', 'loss_hw_dn_unscaled', 'cardinality_error_dn_unscaled', 'loss_ce_unscaled', 'class_error_unscaled', 'loss_bbox_unscaled', 'loss_giou_unscaled', 'loss_xy_unscaled', 'loss_hw_unscaled', 'cardinality_error_unscaled', 'loss_ce_0_unscaled', 'loss_bbox_0_unscaled', 'loss_giou_0_unscaled', 'loss_xy_0_unscaled', 'loss_hw_0_unscaled', 'cardinality_error_0_unscaled', 'loss_bbox_dn_0_unscaled', 'loss_giou_dn_0_unscaled', 'loss_ce_dn_0_unscaled', 'loss_xy_dn_0_unscaled', 'loss_hw_dn_0_unscaled', 'cardinality_error_dn_0_unscaled', 'loss_ce_1_unscaled', 'loss_bbox_1_unscaled', 'loss_giou_1_unscaled', 'loss_xy_1_unscaled', 'loss_hw_1_unscaled', 'cardinality_error_1_unscaled', 'loss_bbox_dn_1_unscaled', 'loss_giou_dn_1_unscaled', 'loss_ce_dn_1_unscaled', 'loss_xy_dn_1_unscaled', 'loss_hw_dn_1_unscaled', 'cardinality_error_dn_1_unscaled', 'loss_ce_2_unscaled', 'loss_bbox_2_unscaled', 'loss_giou_2_unscaled', 'loss_xy_2_unscaled', 'loss_hw_2_unscaled', 'cardinality_error_2_unscaled', 'loss_bbox_dn_2_unscaled', 'loss_giou_dn_2_unscaled', 'loss_ce_dn_2_unscaled', 'loss_xy_dn_2_unscaled', 'loss_hw_dn_2_unscaled', 'cardinality_error_dn_2_unscaled', 'loss_ce_3_unscaled', 'loss_bbox_3_unscaled', 'loss_giou_3_unscaled', 'loss_xy_3_unscaled', 'loss_hw_3_unscaled', 'cardinality_error_3_unscaled', 'loss_bbox_dn_3_unscaled', 'loss_giou_dn_3_unscaled', 'loss_ce_dn_3_unscaled', 'loss_xy_dn_3_unscaled', 'loss_hw_dn_3_unscaled', 'cardinality_error_dn_3_unscaled', 'loss_ce_4_unscaled', 'loss_bbox_4_unscaled', 'loss_giou_4_unscaled', 'loss_xy_4_unscaled', 'loss_hw_4_unscaled', 'cardinality_error_4_unscaled', 'loss_bbox_dn_4_unscaled', 'loss_giou_dn_4_unscaled', 'loss_ce_dn_4_unscaled', 'loss_xy_dn_4_unscaled', 'loss_hw_dn_4_unscaled', 'cardinality_error_dn_4_unscaled', 'loss_ce_interm_unscaled', 'loss_bbox_interm_unscaled', 'loss_giou_interm_unscaled', 'loss_xy_interm_unscaled', 'loss_hw_interm_unscaled', 'cardinality_error_interm_unscaled'])\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + \"0.json\", \"r\") as outfile:\n",
    "    data = json.load(outfile)\n",
    "print(data['input'].keys())\n",
    "print(data['annotation'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64955065-1cd4-4db1-91aa-8b16f4dea443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def read_one_key_from_file(file, first_key, second_key, third_key=None):\n",
    "#     with open(file, \"r\") as outfile:\n",
    "#         data = json.load(outfile)\n",
    "#     return data[first_key][second_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7e36b7-3ad0-46bb-bb98-395337565bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# files_num = 5000\n",
    "# files = os.listdir(data_path)\n",
    "# assert len(files) == files_num\n",
    "# inputs = []\n",
    "# annotations = []\n",
    "# for file_idx in range(files_num):\n",
    "#     file_path = data_path + str(file_idx) + \".json\"\n",
    "#     with open(file_path, \"r\") as outfile:\n",
    "#         data = json.load(outfile)\n",
    "#         pred_logits = np.squeeze(np.array(data['input']['pred_logits']), axis=0)\n",
    "#         pred_boxes = np.squeeze(np.array(data['input']['pred_boxes']), axis=0)\n",
    "#         pred_input = np.concatenate((pred_logits, pred_boxes), axis=1)\n",
    "#         inputs.append(pred_input.tolist())\n",
    "#         loss_ce = data['annotation']['loss_ce']\n",
    "#         loss_bbox = data['annotation']['loss_bbox']\n",
    "#         loss_giou = data['annotation']['loss_giou']\n",
    "#         annotations.append(loss_ce + loss_bbox + loss_giou)\n",
    "#     if file_idx % 100 == 0:\n",
    "#         print(file_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6bd489-0d6d-473c-8f66-4268b63c08de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store_preprocess_inputs_path = model_data_path + split + \"/pre_data/val_inputs.npy\"\n",
    "# with open(store_preprocess_inputs_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, np.array(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52fd912-3c43-4661-9cb1-9ec69c883141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store_preprocess_annotations_path = model_data_path + split + \"/pre_data/val_annotations.npy\"\n",
    "# with open(store_preprocess_annotations_path, \"wb\") as outfile:\n",
    "#     np.save(outfile, np.array(annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119869e3-f1d6-4f98-9ec5-73cbb774fd68",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c792756-c148-4f24-ac03-2230f97e29be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MPLNet(nn.Module):\n",
    "    def __init__(self, input_dims = 10000, output_dims = 1, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer1=nn.Linear(input_dims, 10000)\n",
    "        self.layer2=nn.Linear(10000, 1000)\n",
    "        self.layer3=nn.Linear(1000, 100)\n",
    "        self.layer4=nn.Linear(100, 10)\n",
    "        self.layer5=nn.Linear(10, 1)\n",
    "        self.norm1 = nn.LayerNorm(10000)\n",
    "        self.norm2 = nn.LayerNorm(1000)\n",
    "        self.norm3 = nn.LayerNorm(100)\n",
    "        self.norm4 = nn.LayerNorm(10)\n",
    "        self._init_parms(self.layer1)\n",
    "        self._init_parms(self.layer2)\n",
    "        self._init_parms(self.layer3)\n",
    "        self._init_parms(self.layer4)\n",
    "        self._init_parms(self.layer5)\n",
    "        # self.dropout1 = nn.Dropout(dropout)\n",
    "        # self.dropout2 = nn.Dropout(dropout)\n",
    "        # self.dropout3 = nn.Dropout(dropout)\n",
    "        self.input_dims = input_dims\n",
    "    \n",
    "    def _init_parms(self, module):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, self.input_dims)\n",
    "        x=nn.functional.relu(self.norm1(self.layer1(x)))\n",
    "        x=nn.functional.relu(self.norm2(self.layer2(x)))\n",
    "        x=nn.functional.relu(self.norm3(self.layer3(x)))\n",
    "        x=nn.functional.relu(self.norm4(self.layer4(x)))\n",
    "        x=self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c1131a-98c2-4051-8937-3253c154604b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPLNet(\n",
       "  (layer1): Linear(in_features=85500, out_features=10000, bias=True)\n",
       "  (layer2): Linear(in_features=10000, out_features=1000, bias=True)\n",
       "  (layer3): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (layer4): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (layer5): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (norm1): LayerNorm((10000,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm4): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_name = \"cuda:1\"\n",
    "device = torch.device(device_name)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "rate_learning = 1e-4\n",
    "model = MPLNet(input_dims=85500)\n",
    "loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=rate_learning)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec897e5-8bfb-447b-8f9a-14907a2cd468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "batch_size = 10\n",
    "store_preprocess_inputs_path = model_data_path + split + \"/pre_data/val_inputs.npy\"\n",
    "store_preprocess_annotations_path = model_data_path + split + \"/pre_data/val_annotations.npy\"\n",
    "with open(store_preprocess_inputs_path, 'rb') as outfile:\n",
    "    inputs = torch.from_numpy(np.load(outfile))\n",
    "with open(store_preprocess_annotations_path, 'rb') as outfile:\n",
    "    annotations = torch.from_numpy(np.load(outfile))\n",
    "    \n",
    "datasets = TensorDataset(inputs, annotations)\n",
    "datasets_nums = inputs.shape[0]\n",
    "train_nums = int(datasets_nums * 0.7)\n",
    "train_datasets, val_datasets = torch.utils.data.random_split(datasets, [train_nums, datasets_nums-train_nums])\n",
    "train_sampler = torch.utils.data.RandomSampler(train_datasets)\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(train_sampler, batch_size, drop_last=True)\n",
    "train_dataloader = DataLoader(train_datasets, batch_sampler=batch_sampler_train)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_datasets)\n",
    "valid_dataloader = DataLoader(val_datasets, 1, sampler=val_sampler, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9532b4e8-5b7a-4c26-bdf3-b7268cd00894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1, 10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Step: 0, Loss 1.4659523963928223\n",
      "Train Epoch: 0, Step: 100, Loss 0.05399113893508911\n",
      "Train Epoch: 0, Step: 200, Loss 0.18286354839801788\n",
      "Train Epoch: 0, Step: 300, Loss 0.06545349955558777\n",
      "Epoch: 0\n",
      "Training loss: 0.2372349515557289\n",
      "Validation loss: 0.15801718249951247\n",
      "Train Epoch: 1, Step: 0, Loss 0.031024688854813576\n",
      "Train Epoch: 1, Step: 100, Loss 0.0839051604270935\n",
      "Train Epoch: 1, Step: 200, Loss 0.31396323442459106\n",
      "Train Epoch: 1, Step: 300, Loss 0.17906448245048523\n",
      "Epoch: 1\n",
      "Training loss: 0.16389689891998258\n",
      "Validation loss: 0.16586352959494116\n",
      "Train Epoch: 2, Step: 0, Loss 0.10907010734081268\n",
      "Train Epoch: 2, Step: 100, Loss 0.18140292167663574\n",
      "Train Epoch: 2, Step: 200, Loss 0.14571760594844818\n",
      "Train Epoch: 2, Step: 300, Loss 0.38280653953552246\n",
      "Epoch: 2\n",
      "Training loss: 0.16617160986576762\n",
      "Validation loss: 0.16548182156589106\n",
      "Train Epoch: 3, Step: 0, Loss 0.11528050899505615\n",
      "Train Epoch: 3, Step: 100, Loss 0.10774984210729599\n",
      "Train Epoch: 3, Step: 200, Loss 1.0817832946777344\n",
      "Train Epoch: 3, Step: 300, Loss 0.15541037917137146\n",
      "Epoch: 3\n",
      "Training loss: 0.16192935911406364\n",
      "Validation loss: 0.15595831501902477\n",
      "Train Epoch: 4, Step: 0, Loss 0.14438465237617493\n",
      "Train Epoch: 4, Step: 100, Loss 0.2241121232509613\n",
      "Train Epoch: 4, Step: 200, Loss 0.06779687106609344\n",
      "Train Epoch: 4, Step: 300, Loss 0.04030417650938034\n",
      "Epoch: 4\n",
      "Training loss: 0.1583806599676609\n",
      "Validation loss: 0.15828897108602227\n",
      "Train Epoch: 5, Step: 0, Loss 0.2243189960718155\n",
      "Train Epoch: 5, Step: 100, Loss 0.15107017755508423\n",
      "Train Epoch: 5, Step: 200, Loss 0.11248411238193512\n",
      "Train Epoch: 5, Step: 300, Loss 0.10527728497982025\n",
      "Epoch: 5\n",
      "Training loss: 0.1588573199723448\n",
      "Validation loss: 0.15715949793357178\n",
      "Train Epoch: 6, Step: 0, Loss 0.058887872844934464\n",
      "Train Epoch: 6, Step: 100, Loss 0.21797221899032593\n",
      "Train Epoch: 6, Step: 200, Loss 0.20429660379886627\n",
      "Train Epoch: 6, Step: 300, Loss 0.1572352796792984\n",
      "Epoch: 6\n",
      "Training loss: 0.15739365354978613\n",
      "Validation loss: 0.15761099186033833\n",
      "Train Epoch: 7, Step: 0, Loss 0.09758258610963821\n",
      "Train Epoch: 7, Step: 100, Loss 0.0740179643034935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3879/2179073195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Epoch: {}, Step: {}, Loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    # Performing Training for each epoch\n",
    "    training_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    # The training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input, label = batch\n",
    "        input = input.to(device=device_name).type(torch.float)\n",
    "        label = label.to(device=device_name).type(torch.float)[None, :]\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print(\"Train Epoch: {}, Step: {}, Loss {}\".format(epoch, step, loss.item()))\n",
    "\n",
    "\n",
    "    # Performing Validation for each epoch\n",
    "    validation_loss = 0.\n",
    "    model.eval()\n",
    "\n",
    "    # The validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        input, label = batch\n",
    "        input = input.to(device=device_name).type(torch.float)\n",
    "        label = label.to(device=device_name).type(torch.float)[None, :]\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "    # Calculating the average training and validation loss over epoch\n",
    "    training_loss_avg = training_loss/len(train_dataloader)\n",
    "    validation_loss_avg = validation_loss/len(valid_dataloader)\n",
    "\n",
    "    # Printing average training and average validation losses\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    print(\"Training loss: {}\".format(training_loss_avg))\n",
    "    print(\"Validation loss: {}\".format(validation_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cd72ffe-7191-409d-a02d-7aaaf0dd1d66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6111]], device='cuda:1', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "input = inputs[10].to(device=device_name).type(torch.float)\n",
    "output = model(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f112db-b1fd-4687-becd-4c1ab000c077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d73f8bf-d873-4acf-9d86-68e97fa45430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ckpts/5_scale_31_mpl.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b2f33-8c33-422c-91c4-70cca4efc0b9",
   "metadata": {},
   "source": [
    "## Active testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50aa81cb-cf60-4f45-a23d-5c1400ed9ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights\n",
    "\n",
    "def get_expected_loss():\n",
    "    model.eval()\n",
    "    datasets_nums = inputs.shape[0]\n",
    "    expected_loss = np.zeros(datasets_nums)\n",
    "    for i in range(datasets_nums):\n",
    "        input = inputs[i].to(device=device_name).type(torch.float)\n",
    "        output = model(input)\n",
    "        expected_loss[i] = output.detach().cpu().numpy()\n",
    "    return expected_loss\n",
    "\n",
    "def run_one_approximate_risk_estimator(true_losses, expected_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pick_sample_idxs, weights = acquire(expected_losses, samples_num)\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, expected_losses.size)\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk\n",
    "\n",
    "def get_true_loss():\n",
    "    return annotations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65f87653-7e15-4230-b862-0fa567a3eaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_losses = get_true_loss()\n",
    "expected_losses = get_expected_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb087e9e-4659-42fa-a3fa-6cbbcff2a589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_json_path = \"./results/MPL_loss_compare/\"\n",
    "sample_size_set = [50, 100, 150, 200, 250, 500, 750, 1000, 1500, 2000, 3000]\n",
    "random_seed_set = [4519, 9524, 5901, 1028, 6382, 5383, 5095, 7635,  890,  608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7245d656-1866-46c7-9bfd-88be3adfc47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"MPL_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"MPL\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_approximate_risk_estimator(true_losses, expected_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7b722-f249-4594-8ba7-48f3e5921b88",
   "metadata": {},
   "source": [
    "## Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "722997e5-2101-4909-8ac0-ff3cc42b5c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    return sampled_true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "290436a8-ec75-4ec2-8918-a9cf5c395177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator(true_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26b4becf-987f-4bbc-bf7b-51ec5ecf457b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_one_random_sample_risk_estimator_LURE(true_losses, seed, samples_num):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    sampled_true_losses = true_losses[pick_sample_idxs]\n",
    "    weights = np.ones(samples_num) * 1.0 / samples_num\n",
    "    risk_estimator_weights = LURE_weights_for_risk_estimator(weights, true_losses.size)\n",
    "    loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "    return loss_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "822bb53c-a5fe-4ae7-8ea9-d3e89299c699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_LURE_R50_31_10_runs.json\"\n",
    "json_object = {}\n",
    "for sample_size in sample_size_set:\n",
    "    for seed in random_seed_set:\n",
    "        result = {\"active_test_type\": \"random sample LURE\", \"sample_size\": sample_size}\n",
    "        loss_risk = run_one_random_sample_risk_estimator_LURE(true_losses, seed, sample_size)\n",
    "        result[\"loss\"] = loss_risk\n",
    "        json_object[len(json_object)] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b271f4-eb67-4279-a6a6-68096a4d2bde",
   "metadata": {},
   "source": [
    "## Whole data set risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff46eefe-cf3c-48fa-8d69-66c53621921a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return true_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c094959f-4adb-4e05-bdb3-c3d99acac115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None_R50_31.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": inputs.shape[0]}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(true_losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "with open(file_path, \"w\") as outfile:\n",
    "    json.dump(json_object, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935c059-8c94-4a19-9ec2-f446480601ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
